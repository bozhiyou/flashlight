//
// Generated by LLVM NVPTX Back-End
//

.version 8.4
.target sm_80
.address_size 64

	// .globl	triton_
.extern .shared .align 16 .b8 global_smem[];
.global .align 1 .b8 _$_str[11] = {95, 95, 67, 85, 68, 65, 95, 70, 84, 90};

.visible .entry triton_(
	.param .u64 triton__param_0,
	.param .u64 triton__param_1,
	.param .u64 triton__param_2,
	.param .u64 triton__param_3,
	.param .u64 triton__param_4,
	.param .u64 triton__param_5,
	.param .u64 triton__param_6,
	.param .u64 triton__param_7,
	.param .u64 triton__param_8
)
.maxntid 128, 1, 1
{
	.reg .pred 	%p<61>;
	.reg .b16 	%rs<129>;
	.reg .b32 	%r<1887>;
	.reg .f32 	%f<1919>;
	.reg .b64 	%rd<142>;
	.loc	1 17 0
$L__func_begin0:
	.loc	1 17 0

	ld.param.u64 	%rd14, [triton__param_8];
	ld.param.u64 	%rd13, [triton__param_2];
	ld.param.u64 	%rd12, [triton__param_1];
	ld.param.u64 	%rd41, [triton__param_0];
$L__tmp0:
	.loc	1 84 28
	// begin inline asm
	mov.u32 %r57, %ctaid.x;
	// end inline asm
	.loc	1 85 26
	// begin inline asm
	mov.u32 %r58, %ctaid.y;
	// end inline asm
	.loc	1 85 32
	shr.s32 	%r125, %r58, 31;
	shr.u32 	%r126, %r125, 27;
	add.s32 	%r127, %r58, %r126;
	ld.param.u64 	%rd42, [triton__param_4];
	and.b32  	%r128, %r127, -32;
	ld.param.u64 	%rd43, [triton__param_5];
	sub.s32 	%r129, %r58, %r128;
	.loc	1 90 23
	shl.b32 	%r130, %r127, 15;
	and.b32  	%r131, %r130, -1048576;
	.loc	1 90 44
	shl.b32 	%r132, %r129, 15;
	.loc	1 90 35
	add.s32 	%r133, %r131, %r132;
	.loc	1 94 12
	cvt.s64.s32 	%rd1, %r133;
	mul.wide.s32 	%rd44, %r133, 2;
	add.s64 	%rd45, %rd41, %rd44;
	.loc	1 95 12
	add.s64 	%rd46, %rd12, %rd44;
	.loc	1 96 12
	add.s64 	%rd47, %rd13, %rd44;
	.loc	1 115 23
	shl.b32 	%r1, %r57, 7;
	.loc	1 115 46
	mov.u32 	%r2, %tid.x;
	and.b32  	%r3, %r2, 31;
	shr.u32 	%r4, %r2, 5;
	bfe.u32 	%r134, %r2, 3, 4;
	or.b32  	%r6, %r134, 16;
	or.b32  	%r7, %r134, 32;
	or.b32  	%r8, %r134, 48;
	or.b32  	%r9, %r134, 64;
	or.b32  	%r10, %r134, 80;
	or.b32  	%r11, %r134, 96;
	or.b32  	%r12, %r134, 112;
	.loc	1 119 83
	shr.s32 	%r136, %r57, 31;
	shr.u32 	%r137, %r136, 9;
	add.s32 	%r138, %r57, %r137;
	shr.s32 	%r139, %r138, 23;
	.loc	1 128 8
	cvt.s64.s32 	%rd48, %r1;
	cvt.u64.u32 	%rd2, %r134;
	cvt.u64.u32 	%rd49, %r6;
	cvt.u64.u32 	%rd50, %r7;
	cvt.u64.u32 	%rd51, %r8;
	cvt.u64.u32 	%rd52, %r9;
	cvt.u64.u32 	%rd53, %r10;
	cvt.u64.u32 	%rd54, %r11;
	cvt.u64.u32 	%rd55, %r12;
	.loc	1 133 20
	or.b64  	%rd56, %rd48, %rd2;
	or.b64  	%rd57, %rd48, %rd49;
	or.b64  	%rd58, %rd48, %rd50;
	or.b64  	%rd59, %rd48, %rd51;
	or.b64  	%rd60, %rd48, %rd52;
	or.b64  	%rd61, %rd48, %rd53;
	or.b64  	%rd62, %rd48, %rd54;
	or.b64  	%rd63, %rd48, %rd55;
	shl.b32 	%r140, %r2, 3;
	and.b32  	%r15, %r140, 56;
	shl.b32 	%r141, %r2, 1;
	shl.b64 	%rd64, %rd56, 7;
	mul.wide.u32 	%rd65, %r15, 2;
	or.b64  	%rd66, %rd64, %rd65;
	add.s64 	%rd15, %rd45, %rd66;
	shl.b64 	%rd67, %rd57, 7;
	or.b64  	%rd68, %rd67, %rd65;
	add.s64 	%rd16, %rd45, %rd68;
	shl.b64 	%rd69, %rd58, 7;
	or.b64  	%rd70, %rd69, %rd65;
	add.s64 	%rd17, %rd45, %rd70;
	shl.b64 	%rd71, %rd59, 7;
	or.b64  	%rd72, %rd71, %rd65;
	add.s64 	%rd18, %rd45, %rd72;
	shl.b64 	%rd73, %rd60, 7;
	or.b64  	%rd74, %rd73, %rd65;
	add.s64 	%rd19, %rd45, %rd74;
	shl.b64 	%rd75, %rd61, 7;
	or.b64  	%rd76, %rd75, %rd65;
	add.s64 	%rd20, %rd45, %rd76;
	shl.b64 	%rd77, %rd62, 7;
	or.b64  	%rd78, %rd77, %rd65;
	add.s64 	%rd21, %rd45, %rd78;
	shl.b64 	%rd79, %rd63, 7;
	or.b64  	%rd80, %rd79, %rd65;
	add.s64 	%rd22, %rd45, %rd80;
	mov.pred 	%p1, -1;
	// begin inline asm
	mov.u32 %r59, 0x0;
	mov.u32 %r60, 0x0;
	mov.u32 %r61, 0x0;
	mov.u32 %r62, 0x0;
	@%p1 ld.global.v4.b32 { %r59, %r60, %r61, %r62 }, [ %rd15 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r63, 0x0;
	mov.u32 %r64, 0x0;
	mov.u32 %r65, 0x0;
	mov.u32 %r66, 0x0;
	@%p1 ld.global.v4.b32 { %r63, %r64, %r65, %r66 }, [ %rd16 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r67, 0x0;
	mov.u32 %r68, 0x0;
	mov.u32 %r69, 0x0;
	mov.u32 %r70, 0x0;
	@%p1 ld.global.v4.b32 { %r67, %r68, %r69, %r70 }, [ %rd17 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r71, 0x0;
	mov.u32 %r72, 0x0;
	mov.u32 %r73, 0x0;
	mov.u32 %r74, 0x0;
	@%p1 ld.global.v4.b32 { %r71, %r72, %r73, %r74 }, [ %rd18 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r75, 0x0;
	mov.u32 %r76, 0x0;
	mov.u32 %r77, 0x0;
	mov.u32 %r78, 0x0;
	@%p1 ld.global.v4.b32 { %r75, %r76, %r77, %r78 }, [ %rd19 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r79, 0x0;
	mov.u32 %r80, 0x0;
	mov.u32 %r81, 0x0;
	mov.u32 %r82, 0x0;
	@%p1 ld.global.v4.b32 { %r79, %r80, %r81, %r82 }, [ %rd20 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r83, 0x0;
	mov.u32 %r84, 0x0;
	mov.u32 %r85, 0x0;
	mov.u32 %r86, 0x0;
	@%p1 ld.global.v4.b32 { %r83, %r84, %r85, %r86 }, [ %rd21 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u32 %r87, 0x0;
	mov.u32 %r88, 0x0;
	mov.u32 %r89, 0x0;
	mov.u32 %r90, 0x0;
	@%p1 ld.global.v4.b32 { %r87, %r88, %r89, %r90 }, [ %rd22 + 0 ];
	// end inline asm
	shl.b32 	%r166, %r134, 6;
	xor.b32  	%r167, %r140, %r2;
	and.b32  	%r168, %r167, 56;
	or.b32  	%r16, %r166, %r168;
	shl.b32 	%r169, %r16, 1;
	mov.u32 	%r170, global_smem;
	add.s32 	%r171, %r170, %r169;
	shl.b32 	%r172, %r6, 6;
	or.b32  	%r17, %r172, %r168;
	shl.b32 	%r173, %r17, 1;
	add.s32 	%r174, %r170, %r173;
	shl.b32 	%r175, %r7, 6;
	or.b32  	%r18, %r175, %r168;
	shl.b32 	%r176, %r18, 1;
	add.s32 	%r177, %r170, %r176;
	shl.b32 	%r178, %r8, 6;
	or.b32  	%r19, %r178, %r168;
	shl.b32 	%r179, %r19, 1;
	add.s32 	%r180, %r170, %r179;
	shl.b32 	%r181, %r9, 7;
	shl.b32 	%r182, %r168, 1;
	or.b32  	%r183, %r181, %r182;
	add.s32 	%r184, %r170, %r183;
	shl.b32 	%r185, %r10, 7;
	or.b32  	%r186, %r185, %r182;
	add.s32 	%r187, %r170, %r186;
	shl.b32 	%r188, %r11, 7;
	or.b32  	%r189, %r188, %r182;
	add.s32 	%r190, %r170, %r189;
	shl.b32 	%r191, %r12, 7;
	or.b32  	%r192, %r191, %r182;
	add.s32 	%r193, %r170, %r192;
	st.shared.v4.b32 	[%r171], {%r59, %r60, %r61, %r62};
	st.shared.v4.b32 	[%r174], {%r63, %r64, %r65, %r66};
	st.shared.v4.b32 	[%r177], {%r67, %r68, %r69, %r70};
	st.shared.v4.b32 	[%r180], {%r71, %r72, %r73, %r74};
	st.shared.v4.b32 	[%r184], {%r75, %r76, %r77, %r78};
	st.shared.v4.b32 	[%r187], {%r79, %r80, %r81, %r82};
	st.shared.v4.b32 	[%r190], {%r83, %r84, %r85, %r86};
	st.shared.v4.b32 	[%r193], {%r87, %r88, %r89, %r90};
	.loc	1 141 26
	mul.wide.s32 	%rd81, %r139, 4;
	add.s64 	%rd23, %rd43, %rd81;
	.loc	1 142 23
	// begin inline asm
	mov.u32 %r91, 0x0;
	@%p1 ld.global.b32 { %r91 }, [ %rd23 + 0 ];
	// end inline asm
	.loc	1 142 37
	shl.b32 	%r202, %r91, 30;
	.loc	1 143 42
	add.s64 	%rd24, %rd42, %rd81;
	.loc	1 143 28
	// begin inline asm
	mov.u32 %r92, 0x0;
	@%p1 ld.global.b32 { %r92 }, [ %rd24 + 0 ];
	// end inline asm
	.loc	1 144 45
	shl.b32 	%r203, %r92, 24;
	.loc	1 152 8
	cvt.s64.s32 	%rd4, %r202;
$L__tmp1:
	.loc	1 281 40
	setp.lt.s32 	%p27, %r203, 1;
	setp.gt.s32 	%p28, %r203, 0;
	.loc	1 364 20
	or.b64  	%rd82, %rd4, %rd2;
	or.b64  	%rd83, %rd4, %rd49;
	or.b64  	%rd84, %rd4, %rd50;
	or.b64  	%rd85, %rd4, %rd51;
	shl.b64 	%rd86, %rd82, 7;
	or.b64  	%rd87, %rd86, %rd65;
	add.s64 	%rd25, %rd46, %rd87;
	shl.b64 	%rd88, %rd83, 7;
	or.b64  	%rd89, %rd88, %rd65;
	add.s64 	%rd26, %rd46, %rd89;
	shl.b64 	%rd90, %rd84, 7;
	or.b64  	%rd91, %rd90, %rd65;
	add.s64 	%rd27, %rd46, %rd91;
	shl.b64 	%rd92, %rd85, 7;
	or.b64  	%rd93, %rd92, %rd65;
	add.s64 	%rd28, %rd46, %rd93;
	add.s32 	%r205, %r170, 16384;
	add.s32 	%r93, %r205, %r169;
	add.s32 	%r95, %r205, %r173;
	add.s32 	%r97, %r205, %r176;
	add.s32 	%r99, %r205, %r179;
	selp.b32 	%r94, 16, 0, %p28;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r93 + 0 ], [ %rd25 + 0 ], 0x10, %r94;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r95 + 0 ], [ %rd26 + 0 ], 0x10, %r94;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r97 + 0 ], [ %rd27 + 0 ], 0x10, %r94;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r99 + 0 ], [ %rd28 + 0 ], 0x10, %r94;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 437 20
	add.s64 	%rd29, %rd47, %rd87;
	add.s64 	%rd30, %rd47, %rd89;
	add.s64 	%rd31, %rd47, %rd91;
	add.s64 	%rd32, %rd47, %rd93;
	add.s32 	%r206, %r170, 32768;
	add.s32 	%r101, %r206, %r169;
	add.s32 	%r103, %r206, %r173;
	add.s32 	%r105, %r206, %r176;
	add.s32 	%r107, %r206, %r179;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r101 + 0 ], [ %rd29 + 0 ], 0x10, %r94;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r103 + 0 ], [ %rd30 + 0 ], 0x10, %r94;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r105 + 0 ], [ %rd31 + 0 ], 0x10, %r94;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r107 + 0 ], [ %rd32 + 0 ], 0x10, %r94;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 281 40
	setp.gt.s32 	%p29, %r203, 1;
	.loc	1 315 46
	or.b64  	%rd94, %rd4, 64;
	.loc	1 364 20
	or.b64  	%rd95, %rd94, %rd2;
	or.b64  	%rd96, %rd94, %rd49;
	or.b64  	%rd97, %rd94, %rd50;
	or.b64  	%rd98, %rd94, %rd51;
	shl.b64 	%rd99, %rd95, 7;
	or.b64  	%rd100, %rd99, %rd65;
	add.s64 	%rd33, %rd46, %rd100;
	shl.b64 	%rd101, %rd96, 7;
	or.b64  	%rd102, %rd101, %rd65;
	add.s64 	%rd34, %rd46, %rd102;
	shl.b64 	%rd103, %rd97, 7;
	or.b64  	%rd104, %rd103, %rd65;
	add.s64 	%rd35, %rd46, %rd104;
	shl.b64 	%rd105, %rd98, 7;
	or.b64  	%rd106, %rd105, %rd65;
	add.s64 	%rd36, %rd46, %rd106;
	bar.sync 	0;
	add.s32 	%r207, %r170, 24576;
	add.s32 	%r109, %r207, %r169;
	add.s32 	%r111, %r207, %r173;
	add.s32 	%r113, %r207, %r176;
	add.s32 	%r115, %r207, %r179;
	selp.b32 	%r110, 16, 0, %p29;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r109 + 0 ], [ %rd33 + 0 ], 0x10, %r110;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r111 + 0 ], [ %rd34 + 0 ], 0x10, %r110;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r113 + 0 ], [ %rd35 + 0 ], 0x10, %r110;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r115 + 0 ], [ %rd36 + 0 ], 0x10, %r110;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 437 20
	add.s64 	%rd37, %rd47, %rd100;
	add.s64 	%rd38, %rd47, %rd102;
	add.s64 	%rd39, %rd47, %rd104;
	add.s64 	%rd40, %rd47, %rd106;
	add.s32 	%r208, %r170, 40960;
	add.s32 	%r117, %r208, %r169;
	add.s32 	%r119, %r208, %r173;
	add.s32 	%r121, %r208, %r176;
	add.s32 	%r123, %r208, %r179;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r117 + 0 ], [ %rd37 + 0 ], 0x10, %r110;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r119 + 0 ], [ %rd38 + 0 ], 0x10, %r110;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r121 + 0 ], [ %rd39 + 0 ], 0x10, %r110;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r123 + 0 ], [ %rd40 + 0 ], 0x10, %r110;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 364 20
	// begin inline asm
	cp.async.wait_group 0x2;
	// end inline asm
	bar.sync 	0;
$L__tmp2:
	.loc	1 0 0
	and.b32  	%r22, %r141, 6;
	mov.f32 	%f870, 0f00000000;
	mov.f32 	%f871, %f870;
	mov.f32 	%f872, %f870;
	mov.f32 	%f873, %f870;
	mov.f32 	%f878, %f870;
	mov.f32 	%f879, %f870;
	mov.f32 	%f880, %f870;
	mov.f32 	%f881, %f870;
	mov.f32 	%f886, %f870;
	mov.f32 	%f887, %f870;
	mov.f32 	%f888, %f870;
	mov.f32 	%f889, %f870;
	mov.f32 	%f894, %f870;
	mov.f32 	%f895, %f870;
	mov.f32 	%f896, %f870;
	mov.f32 	%f897, %f870;
	mov.f32 	%f902, %f870;
	mov.f32 	%f903, %f870;
	mov.f32 	%f904, %f870;
	mov.f32 	%f905, %f870;
	mov.f32 	%f910, %f870;
	mov.f32 	%f911, %f870;
	mov.f32 	%f912, %f870;
	mov.f32 	%f913, %f870;
	mov.f32 	%f918, %f870;
	mov.f32 	%f919, %f870;
	mov.f32 	%f920, %f870;
	mov.f32 	%f921, %f870;
	mov.f32 	%f926, %f870;
	mov.f32 	%f927, %f870;
	mov.f32 	%f928, %f870;
	mov.f32 	%f929, %f870;
	mov.f32 	%f934, %f870;
	mov.f32 	%f935, %f870;
	mov.f32 	%f936, %f870;
	mov.f32 	%f937, %f870;
	mov.f32 	%f942, %f870;
	mov.f32 	%f943, %f870;
	mov.f32 	%f944, %f870;
	mov.f32 	%f945, %f870;
	mov.f32 	%f950, %f870;
	mov.f32 	%f951, %f870;
	mov.f32 	%f952, %f870;
	mov.f32 	%f953, %f870;
	mov.f32 	%f958, %f870;
	mov.f32 	%f959, %f870;
	mov.f32 	%f960, %f870;
	mov.f32 	%f961, %f870;
	mov.f32 	%f966, %f870;
	mov.f32 	%f967, %f870;
	mov.f32 	%f968, %f870;
	mov.f32 	%f969, %f870;
	mov.f32 	%f974, %f870;
	mov.f32 	%f975, %f870;
	mov.f32 	%f976, %f870;
	mov.f32 	%f977, %f870;
	mov.f32 	%f982, %f870;
	mov.f32 	%f983, %f870;
	mov.f32 	%f984, %f870;
	mov.f32 	%f985, %f870;
	mov.f32 	%f990, %f870;
	mov.f32 	%f991, %f870;
	mov.f32 	%f992, %f870;
	mov.f32 	%f993, %f870;
	mov.f32 	%f1915, %f870;
	mov.f32 	%f1916, %f870;
	mov.f32 	%f1917, %f870;
	mov.f32 	%f1918, %f870;
$L__tmp3:
	.loc	1 281 40
	@%p27 bra 	$L__BB0_3;
	.loc	1 0 40
	shr.u32 	%r5, %r2, 3;
	bfe.u32 	%r13, %r2, 2, 3;
	shr.u32 	%r135, %r2, 1;
	and.b32  	%r14, %r135, 48;
	min.s32 	%r21, %r203, 8;
	add.s32 	%r204, %r129, 1;
	cvt.rn.f32.s32 	%f219, %r204;
	mul.f32 	%f220, %f219, 0fC1000000;
	fma.rn.f32 	%f1, %f220, 0f3D000000, 0f00000000;
	cvt.u32.u64 	%r213, %rd4;
	add.s32 	%r23, %r21, -2;
	add.s32 	%r24, %r21, -1;
	and.b32  	%r214, %r2, 7;
	and.b32  	%r215, %r5, 1;
	shr.u32 	%r216, %r3, 4;
	shl.b32 	%r217, %r4, 1;
	and.b32  	%r218, %r217, 6;
	or.b32  	%r219, %r218, %r215;
	xor.b32  	%r220, %r216, %r214;
	shl.b32 	%r221, %r219, 9;
	shl.b32 	%r222, %r214, 6;
	or.b32  	%r223, %r221, %r222;
	shl.b32 	%r224, %r220, 3;
	or.b32  	%r225, %r223, %r224;
	shl.b32 	%r226, %r225, 1;
	add.s32 	%r266, %r170, %r226;
	or.b32  	%r228, %r216, 2;
	xor.b32  	%r229, %r228, %r214;
	shl.b32 	%r230, %r229, 3;
	or.b32  	%r231, %r223, %r230;
	shl.b32 	%r232, %r231, 1;
	add.s32 	%r271, %r170, %r232;
	or.b32  	%r233, %r216, 4;
	xor.b32  	%r234, %r233, %r214;
	shl.b32 	%r235, %r234, 3;
	or.b32  	%r236, %r223, %r235;
	shl.b32 	%r237, %r236, 1;
	add.s32 	%r276, %r170, %r237;
	or.b32  	%r238, %r216, 6;
	xor.b32  	%r239, %r238, %r214;
	shl.b32 	%r240, %r239, 3;
	or.b32  	%r241, %r223, %r240;
	shl.b32 	%r242, %r241, 1;
	add.s32 	%r281, %r170, %r242;
	add.s32 	%r286, %r266, 8192;
	add.s32 	%r291, %r271, 8192;
	add.s32 	%r296, %r276, 8192;
	add.s32 	%r301, %r281, 8192;
	or.b32  	%r243, %r215, 2;
	or.b32  	%r244, %r215, 4;
	or.b32  	%r245, %r215, 6;
	xor.b32  	%r246, %r215, %r214;
	shl.b32 	%r247, %r216, 9;
	or.b32  	%r248, %r247, %r222;
	shl.b32 	%r249, %r246, 3;
	or.b32  	%r33, %r249, %r248;
	xor.b32  	%r250, %r243, %r214;
	shl.b32 	%r251, %r250, 3;
	or.b32  	%r34, %r251, %r248;
	xor.b32  	%r252, %r244, %r214;
	shl.b32 	%r253, %r252, 3;
	or.b32  	%r35, %r253, %r248;
	xor.b32  	%r254, %r245, %r214;
	shl.b32 	%r255, %r254, 3;
	or.b32  	%r36, %r255, %r248;
	shl.b32 	%r256, %r2, 6;
	and.b32  	%r257, %r256, 960;
	or.b32  	%r37, %r224, %r257;
	or.b32  	%r38, %r230, %r257;
	or.b32  	%r39, %r235, %r257;
	or.b32  	%r40, %r240, %r257;
	.loc	1 281 40
	sub.s32 	%r258, %r213, %r13;
	sub.s32 	%r259, %r258, %r14;
	sub.s32 	%r1881, %r259, %r1;
	add.s32 	%r260, %r1, %r14;
	add.s32 	%r261, %r260, %r13;
	sub.s32 	%r1880, %r213, %r261;
	shl.b64 	%rd5, %rd1, 1;
	add.s64 	%rd107, %rd4, %rd2;
	shl.b64 	%rd108, %rd107, 7;
	mul.wide.u32 	%rd109, %r214, 16;
	or.b64  	%rd110, %rd108, %rd109;
	add.s64 	%rd141, %rd13, %rd110;
	add.s64 	%rd140, %rd12, %rd110;
	mov.f32 	%f1847, 0fFF800000;
	mov.f32 	%f224, 0f00000000;
	mov.b32 	%r1885, 1;
	mov.b32 	%r1884, 0;
	shl.b32 	%r1312, %r33, 1;
	shl.b32 	%r1313, %r34, 1;
	shl.b32 	%r1314, %r35, 1;
	shl.b32 	%r1315, %r36, 1;
	ex2.approx.ftz.f32 	%f1302, %f1;
	shl.b32 	%r1492, %r37, 1;
	shl.b32 	%r1493, %r38, 1;
	shl.b32 	%r1494, %r39, 1;
	shl.b32 	%r1495, %r40, 1;
	mov.u32 	%r1882, %r206;
	mov.u32 	%r1883, %r205;
	mov.f32 	%f870, %f224;
	mov.f32 	%f871, %f224;
	mov.f32 	%f872, %f224;
	mov.f32 	%f873, %f224;
	mov.f32 	%f878, %f224;
	mov.f32 	%f879, %f224;
	mov.f32 	%f880, %f224;
	mov.f32 	%f881, %f224;
	mov.f32 	%f886, %f224;
	mov.f32 	%f887, %f224;
	mov.f32 	%f888, %f224;
	mov.f32 	%f889, %f224;
	mov.f32 	%f894, %f224;
	mov.f32 	%f895, %f224;
	mov.f32 	%f896, %f224;
	mov.f32 	%f897, %f224;
	mov.f32 	%f902, %f224;
	mov.f32 	%f903, %f224;
	mov.f32 	%f904, %f224;
	mov.f32 	%f905, %f224;
	mov.f32 	%f910, %f224;
	mov.f32 	%f911, %f224;
	mov.f32 	%f912, %f224;
	mov.f32 	%f913, %f224;
	mov.f32 	%f918, %f224;
	mov.f32 	%f919, %f224;
	mov.f32 	%f920, %f224;
	mov.f32 	%f921, %f224;
	mov.f32 	%f926, %f224;
	mov.f32 	%f927, %f224;
	mov.f32 	%f928, %f224;
	mov.f32 	%f929, %f224;
	mov.f32 	%f934, %f224;
	mov.f32 	%f935, %f224;
	mov.f32 	%f936, %f224;
	mov.f32 	%f937, %f224;
	mov.f32 	%f942, %f224;
	mov.f32 	%f943, %f224;
	mov.f32 	%f944, %f224;
	mov.f32 	%f945, %f224;
	mov.f32 	%f950, %f224;
	mov.f32 	%f951, %f224;
	mov.f32 	%f952, %f224;
	mov.f32 	%f953, %f224;
	mov.f32 	%f958, %f224;
	mov.f32 	%f959, %f224;
	mov.f32 	%f960, %f224;
	mov.f32 	%f961, %f224;
	mov.f32 	%f966, %f224;
	mov.f32 	%f967, %f224;
	mov.f32 	%f968, %f224;
	mov.f32 	%f969, %f224;
	mov.f32 	%f974, %f224;
	mov.f32 	%f975, %f224;
	mov.f32 	%f976, %f224;
	mov.f32 	%f977, %f224;
	mov.f32 	%f982, %f224;
	mov.f32 	%f983, %f224;
	mov.f32 	%f984, %f224;
	mov.f32 	%f985, %f224;
	mov.f32 	%f990, %f224;
	mov.f32 	%f991, %f224;
	mov.f32 	%f992, %f224;
	mov.f32 	%f993, %f224;
	mov.u32 	%r1886, %r1884;
	mov.f32 	%f1915, %f224;
	mov.f32 	%f1916, %f224;
	mov.f32 	%f1917, %f224;
	mov.f32 	%f1918, %f224;
	mov.f32 	%f1848, %f1847;
	mov.f32 	%f1849, %f1847;
	mov.f32 	%f1850, %f1847;
$L__BB0_2:
	setp.lt.s32 	%p40, %r1886, %r23;
	setp.lt.s32 	%p30, %r1886, %r24;
$L__tmp4:
	.loc	1 133 20
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r382, %r383, %r384, %r385 }, [ %r266 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r478, %r479, %r480, %r481 }, [ %r271 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r574, %r575, %r576, %r577 }, [ %r276 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r670, %r671, %r672, %r673 }, [ %r281 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r430, %r431, %r432, %r433 }, [ %r286 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r526, %r527, %r528, %r529 }, [ %r291 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r622, %r623, %r624, %r625 }, [ %r296 + 0 ];
	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r718, %r719, %r720, %r721 }, [ %r301 + 0 ];
	// end inline asm
$L__tmp5:
	.loc	1 364 20
	add.s32 	%r306, %r1883, %r1312;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r386, %r387, %r392, %r393 }, [ %r306 + 0 ];
	// end inline asm
	add.s32 	%r311, %r1883, %r1313;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r482, %r483, %r488, %r489 }, [ %r311 + 0 ];
	// end inline asm
	add.s32 	%r316, %r1883, %r1314;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r578, %r579, %r584, %r585 }, [ %r316 + 0 ];
	// end inline asm
	add.s32 	%r321, %r1883, %r1315;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r674, %r675, %r680, %r681 }, [ %r321 + 0 ];
	// end inline asm
	add.s32 	%r326, %r306, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r398, %r399, %r404, %r405 }, [ %r326 + 0 ];
	// end inline asm
	add.s32 	%r331, %r311, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r494, %r495, %r500, %r501 }, [ %r331 + 0 ];
	// end inline asm
	add.s32 	%r336, %r316, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r590, %r591, %r596, %r597 }, [ %r336 + 0 ];
	// end inline asm
	add.s32 	%r341, %r321, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r686, %r687, %r692, %r693 }, [ %r341 + 0 ];
	// end inline asm
	add.s32 	%r346, %r306, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r410, %r411, %r416, %r417 }, [ %r346 + 0 ];
	// end inline asm
	add.s32 	%r351, %r311, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r506, %r507, %r512, %r513 }, [ %r351 + 0 ];
	// end inline asm
	add.s32 	%r356, %r316, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r602, %r603, %r608, %r609 }, [ %r356 + 0 ];
	// end inline asm
	add.s32 	%r361, %r321, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r698, %r699, %r704, %r705 }, [ %r361 + 0 ];
	// end inline asm
	add.s32 	%r366, %r306, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r422, %r423, %r428, %r429 }, [ %r366 + 0 ];
	// end inline asm
	add.s32 	%r371, %r311, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r518, %r519, %r524, %r525 }, [ %r371 + 0 ];
	// end inline asm
	add.s32 	%r376, %r316, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r614, %r615, %r620, %r621 }, [ %r376 + 0 ];
	// end inline asm
	add.s32 	%r381, %r321, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r710, %r711, %r716, %r717 }, [ %r381 + 0 ];
	// end inline asm
	.loc	1 368 19
	mov.f32 	%f358, %f224;
	mov.f32 	%f359, %f224;
	mov.f32 	%f360, %f224;
	mov.f32 	%f361, %f224;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f358, %f359, %f360, %f361 }, { %r382, %r383, %r384, %r385 }, { %r386, %r387 }, { %f358, %f359, %f360, %f361 };
	// end inline asm
	mov.f32 	%f366, %f224;
	mov.f32 	%f367, %f224;
	mov.f32 	%f368, %f224;
	mov.f32 	%f369, %f224;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f366, %f367, %f368, %f369 }, { %r382, %r383, %r384, %r385 }, { %r392, %r393 }, { %f366, %f367, %f368, %f369 };
	// end inline asm
	mov.f32 	%f374, %f224;
	mov.f32 	%f375, %f224;
	mov.f32 	%f376, %f224;
	mov.f32 	%f377, %f224;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f374, %f375, %f376, %f377 }, { %r382, %r383, %r384, %r385 }, { %r398, %r399 }, { %f374, %f375, %f376, %f377 };
	// end inline asm
	mov.f32 	%f382, %f224;
	mov.f32 	%f383, %f224;
	mov.f32 	%f384, %f224;
	mov.f32 	%f385, %f224;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f382, %f383, %f384, %f385 }, { %r382, %r383, %r384, %r385 }, { %r404, %r405 }, { %f382, %f383, %f384, %f385 };
	// end inline asm
	mov.f32 	%f390, %f224;
	mov.f32 	%f391, %f224;
	mov.f32 	%f392, %f224;
	mov.f32 	%f393, %f224;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f390, %f391, %f392, %f393 }, { %r382, %r383, %r384, %r385 }, { %r410, %r411 }, { %f390, %f391, %f392, %f393 };
	// end inline asm
	mov.f32 	%f398, %f224;
	mov.f32 	%f399, %f224;
	mov.f32 	%f400, %f224;
	mov.f32 	%f401, %f224;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f398, %f399, %f400, %f401 }, { %r382, %r383, %r384, %r385 }, { %r416, %r417 }, { %f398, %f399, %f400, %f401 };
	// end inline asm
	mov.f32 	%f406, %f224;
	mov.f32 	%f407, %f224;
	mov.f32 	%f408, %f224;
	mov.f32 	%f409, %f224;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f406, %f407, %f408, %f409 }, { %r382, %r383, %r384, %r385 }, { %r422, %r423 }, { %f406, %f407, %f408, %f409 };
	// end inline asm
	mov.f32 	%f414, %f224;
	mov.f32 	%f415, %f224;
	mov.f32 	%f416, %f224;
	mov.f32 	%f417, %f224;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f414, %f415, %f416, %f417 }, { %r382, %r383, %r384, %r385 }, { %r428, %r429 }, { %f414, %f415, %f416, %f417 };
	// end inline asm
	mov.f32 	%f422, %f224;
	mov.f32 	%f423, %f224;
	mov.f32 	%f424, %f224;
	mov.f32 	%f425, %f224;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f422, %f423, %f424, %f425 }, { %r430, %r431, %r432, %r433 }, { %r386, %r387 }, { %f422, %f423, %f424, %f425 };
	// end inline asm
	mov.f32 	%f430, %f224;
	mov.f32 	%f431, %f224;
	mov.f32 	%f432, %f224;
	mov.f32 	%f433, %f224;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f430, %f431, %f432, %f433 }, { %r430, %r431, %r432, %r433 }, { %r392, %r393 }, { %f430, %f431, %f432, %f433 };
	// end inline asm
	mov.f32 	%f438, %f224;
	mov.f32 	%f439, %f224;
	mov.f32 	%f440, %f224;
	mov.f32 	%f441, %f224;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f438, %f439, %f440, %f441 }, { %r430, %r431, %r432, %r433 }, { %r398, %r399 }, { %f438, %f439, %f440, %f441 };
	// end inline asm
	mov.f32 	%f446, %f224;
	mov.f32 	%f447, %f224;
	mov.f32 	%f448, %f224;
	mov.f32 	%f449, %f224;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f446, %f447, %f448, %f449 }, { %r430, %r431, %r432, %r433 }, { %r404, %r405 }, { %f446, %f447, %f448, %f449 };
	// end inline asm
	mov.f32 	%f454, %f224;
	mov.f32 	%f455, %f224;
	mov.f32 	%f456, %f224;
	mov.f32 	%f457, %f224;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f454, %f455, %f456, %f457 }, { %r430, %r431, %r432, %r433 }, { %r410, %r411 }, { %f454, %f455, %f456, %f457 };
	// end inline asm
	mov.f32 	%f462, %f224;
	mov.f32 	%f463, %f224;
	mov.f32 	%f464, %f224;
	mov.f32 	%f465, %f224;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f462, %f463, %f464, %f465 }, { %r430, %r431, %r432, %r433 }, { %r416, %r417 }, { %f462, %f463, %f464, %f465 };
	// end inline asm
	mov.f32 	%f470, %f224;
	mov.f32 	%f471, %f224;
	mov.f32 	%f472, %f224;
	mov.f32 	%f473, %f224;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f470, %f471, %f472, %f473 }, { %r430, %r431, %r432, %r433 }, { %r422, %r423 }, { %f470, %f471, %f472, %f473 };
	// end inline asm
	mov.f32 	%f478, %f224;
	mov.f32 	%f479, %f224;
	mov.f32 	%f480, %f224;
	mov.f32 	%f481, %f224;
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f478, %f479, %f480, %f481 }, { %r430, %r431, %r432, %r433 }, { %r428, %r429 }, { %f478, %f479, %f480, %f481 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f358, %f359, %f360, %f361 }, { %r478, %r479, %r480, %r481 }, { %r482, %r483 }, { %f358, %f359, %f360, %f361 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f366, %f367, %f368, %f369 }, { %r478, %r479, %r480, %r481 }, { %r488, %r489 }, { %f366, %f367, %f368, %f369 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f374, %f375, %f376, %f377 }, { %r478, %r479, %r480, %r481 }, { %r494, %r495 }, { %f374, %f375, %f376, %f377 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f382, %f383, %f384, %f385 }, { %r478, %r479, %r480, %r481 }, { %r500, %r501 }, { %f382, %f383, %f384, %f385 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f390, %f391, %f392, %f393 }, { %r478, %r479, %r480, %r481 }, { %r506, %r507 }, { %f390, %f391, %f392, %f393 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f398, %f399, %f400, %f401 }, { %r478, %r479, %r480, %r481 }, { %r512, %r513 }, { %f398, %f399, %f400, %f401 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f406, %f407, %f408, %f409 }, { %r478, %r479, %r480, %r481 }, { %r518, %r519 }, { %f406, %f407, %f408, %f409 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f414, %f415, %f416, %f417 }, { %r478, %r479, %r480, %r481 }, { %r524, %r525 }, { %f414, %f415, %f416, %f417 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f422, %f423, %f424, %f425 }, { %r526, %r527, %r528, %r529 }, { %r482, %r483 }, { %f422, %f423, %f424, %f425 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f430, %f431, %f432, %f433 }, { %r526, %r527, %r528, %r529 }, { %r488, %r489 }, { %f430, %f431, %f432, %f433 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f438, %f439, %f440, %f441 }, { %r526, %r527, %r528, %r529 }, { %r494, %r495 }, { %f438, %f439, %f440, %f441 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f446, %f447, %f448, %f449 }, { %r526, %r527, %r528, %r529 }, { %r500, %r501 }, { %f446, %f447, %f448, %f449 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f454, %f455, %f456, %f457 }, { %r526, %r527, %r528, %r529 }, { %r506, %r507 }, { %f454, %f455, %f456, %f457 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f462, %f463, %f464, %f465 }, { %r526, %r527, %r528, %r529 }, { %r512, %r513 }, { %f462, %f463, %f464, %f465 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f470, %f471, %f472, %f473 }, { %r526, %r527, %r528, %r529 }, { %r518, %r519 }, { %f470, %f471, %f472, %f473 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f478, %f479, %f480, %f481 }, { %r526, %r527, %r528, %r529 }, { %r524, %r525 }, { %f478, %f479, %f480, %f481 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f358, %f359, %f360, %f361 }, { %r574, %r575, %r576, %r577 }, { %r578, %r579 }, { %f358, %f359, %f360, %f361 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f366, %f367, %f368, %f369 }, { %r574, %r575, %r576, %r577 }, { %r584, %r585 }, { %f366, %f367, %f368, %f369 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f374, %f375, %f376, %f377 }, { %r574, %r575, %r576, %r577 }, { %r590, %r591 }, { %f374, %f375, %f376, %f377 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f382, %f383, %f384, %f385 }, { %r574, %r575, %r576, %r577 }, { %r596, %r597 }, { %f382, %f383, %f384, %f385 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f390, %f391, %f392, %f393 }, { %r574, %r575, %r576, %r577 }, { %r602, %r603 }, { %f390, %f391, %f392, %f393 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f398, %f399, %f400, %f401 }, { %r574, %r575, %r576, %r577 }, { %r608, %r609 }, { %f398, %f399, %f400, %f401 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f406, %f407, %f408, %f409 }, { %r574, %r575, %r576, %r577 }, { %r614, %r615 }, { %f406, %f407, %f408, %f409 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f414, %f415, %f416, %f417 }, { %r574, %r575, %r576, %r577 }, { %r620, %r621 }, { %f414, %f415, %f416, %f417 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f422, %f423, %f424, %f425 }, { %r622, %r623, %r624, %r625 }, { %r578, %r579 }, { %f422, %f423, %f424, %f425 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f430, %f431, %f432, %f433 }, { %r622, %r623, %r624, %r625 }, { %r584, %r585 }, { %f430, %f431, %f432, %f433 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f438, %f439, %f440, %f441 }, { %r622, %r623, %r624, %r625 }, { %r590, %r591 }, { %f438, %f439, %f440, %f441 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f446, %f447, %f448, %f449 }, { %r622, %r623, %r624, %r625 }, { %r596, %r597 }, { %f446, %f447, %f448, %f449 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f454, %f455, %f456, %f457 }, { %r622, %r623, %r624, %r625 }, { %r602, %r603 }, { %f454, %f455, %f456, %f457 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f462, %f463, %f464, %f465 }, { %r622, %r623, %r624, %r625 }, { %r608, %r609 }, { %f462, %f463, %f464, %f465 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f470, %f471, %f472, %f473 }, { %r622, %r623, %r624, %r625 }, { %r614, %r615 }, { %f470, %f471, %f472, %f473 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f478, %f479, %f480, %f481 }, { %r622, %r623, %r624, %r625 }, { %r620, %r621 }, { %f478, %f479, %f480, %f481 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f358, %f359, %f360, %f361 }, { %r670, %r671, %r672, %r673 }, { %r674, %r675 }, { %f358, %f359, %f360, %f361 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f366, %f367, %f368, %f369 }, { %r670, %r671, %r672, %r673 }, { %r680, %r681 }, { %f366, %f367, %f368, %f369 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f374, %f375, %f376, %f377 }, { %r670, %r671, %r672, %r673 }, { %r686, %r687 }, { %f374, %f375, %f376, %f377 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f382, %f383, %f384, %f385 }, { %r670, %r671, %r672, %r673 }, { %r692, %r693 }, { %f382, %f383, %f384, %f385 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f390, %f391, %f392, %f393 }, { %r670, %r671, %r672, %r673 }, { %r698, %r699 }, { %f390, %f391, %f392, %f393 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f398, %f399, %f400, %f401 }, { %r670, %r671, %r672, %r673 }, { %r704, %r705 }, { %f398, %f399, %f400, %f401 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f406, %f407, %f408, %f409 }, { %r670, %r671, %r672, %r673 }, { %r710, %r711 }, { %f406, %f407, %f408, %f409 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f414, %f415, %f416, %f417 }, { %r670, %r671, %r672, %r673 }, { %r716, %r717 }, { %f414, %f415, %f416, %f417 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f422, %f423, %f424, %f425 }, { %r718, %r719, %r720, %r721 }, { %r674, %r675 }, { %f422, %f423, %f424, %f425 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f430, %f431, %f432, %f433 }, { %r718, %r719, %r720, %r721 }, { %r680, %r681 }, { %f430, %f431, %f432, %f433 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f438, %f439, %f440, %f441 }, { %r718, %r719, %r720, %r721 }, { %r686, %r687 }, { %f438, %f439, %f440, %f441 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f446, %f447, %f448, %f449 }, { %r718, %r719, %r720, %r721 }, { %r692, %r693 }, { %f446, %f447, %f448, %f449 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f454, %f455, %f456, %f457 }, { %r718, %r719, %r720, %r721 }, { %r698, %r699 }, { %f454, %f455, %f456, %f457 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f462, %f463, %f464, %f465 }, { %r718, %r719, %r720, %r721 }, { %r704, %r705 }, { %f462, %f463, %f464, %f465 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f470, %f471, %f472, %f473 }, { %r718, %r719, %r720, %r721 }, { %r710, %r711 }, { %f470, %f471, %f472, %f473 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f478, %f479, %f480, %f481 }, { %r718, %r719, %r720, %r721 }, { %r716, %r717 }, { %f478, %f479, %f480, %f481 };
	// end inline asm
	.loc	1 383 18
	add.s32 	%r1316, %r22, %r1880;
	add.s32 	%r1317, %r1316, 1;
	add.s32 	%r1318, %r22, %r1881;
	add.s32 	%r1319, %r1318, -8;
	add.s32 	%r1320, %r1318, -7;
	add.s32 	%r1321, %r1316, 8;
	add.s32 	%r1322, %r1316, 9;
	add.s32 	%r1323, %r1318, 1;
	add.s32 	%r1324, %r1316, 16;
	add.s32 	%r1325, %r1316, 17;
	add.s32 	%r1326, %r1318, 8;
	add.s32 	%r1327, %r1318, 9;
	add.s32 	%r1328, %r1316, 24;
	add.s32 	%r1329, %r1316, 25;
	add.s32 	%r1330, %r1318, 16;
	add.s32 	%r1331, %r1318, 17;
	add.s32 	%r1332, %r1316, 32;
	add.s32 	%r1333, %r1316, 33;
	add.s32 	%r1334, %r1318, 24;
	add.s32 	%r1335, %r1318, 25;
	add.s32 	%r1336, %r1316, 40;
	add.s32 	%r1337, %r1316, 41;
	add.s32 	%r1338, %r1318, 32;
	add.s32 	%r1339, %r1318, 33;
	add.s32 	%r1340, %r1316, 48;
	add.s32 	%r1341, %r1316, 49;
	add.s32 	%r1342, %r1318, 40;
	add.s32 	%r1343, %r1318, 41;
	add.s32 	%r1344, %r1316, 56;
	add.s32 	%r1345, %r1316, 57;
	add.s32 	%r1346, %r1318, 48;
	add.s32 	%r1347, %r1318, 49;
	add.s32 	%r1348, %r1318, -64;
	add.s32 	%r1349, %r1318, -63;
	add.s32 	%r1350, %r1318, -72;
	add.s32 	%r1351, %r1318, -71;
	add.s32 	%r1352, %r1318, -56;
	add.s32 	%r1353, %r1318, -55;
	add.s32 	%r1354, %r1318, -48;
	add.s32 	%r1355, %r1318, -47;
	add.s32 	%r1356, %r1318, -40;
	add.s32 	%r1357, %r1318, -39;
	add.s32 	%r1358, %r1318, -32;
	add.s32 	%r1359, %r1318, -31;
	add.s32 	%r1360, %r1318, -24;
	add.s32 	%r1361, %r1318, -23;
	add.s32 	%r1362, %r1318, -16;
	add.s32 	%r1363, %r1318, -15;
	.loc	1 384 19
	cvt.rn.f32.s32 	%f1254, %r1316;
	cvt.rn.f32.s32 	%f1255, %r1317;
	cvt.rn.f32.s32 	%f1256, %r1319;
	cvt.rn.f32.s32 	%f1257, %r1320;
	cvt.rn.f32.s32 	%f1258, %r1321;
	cvt.rn.f32.s32 	%f1259, %r1322;
	cvt.rn.f32.s32 	%f1260, %r1318;
	cvt.rn.f32.s32 	%f1261, %r1323;
	cvt.rn.f32.s32 	%f1262, %r1324;
	cvt.rn.f32.s32 	%f1263, %r1325;
	cvt.rn.f32.s32 	%f1264, %r1326;
	cvt.rn.f32.s32 	%f1265, %r1327;
	cvt.rn.f32.s32 	%f1266, %r1328;
	cvt.rn.f32.s32 	%f1267, %r1329;
	cvt.rn.f32.s32 	%f1268, %r1330;
	cvt.rn.f32.s32 	%f1269, %r1331;
	cvt.rn.f32.s32 	%f1270, %r1332;
	cvt.rn.f32.s32 	%f1271, %r1333;
	cvt.rn.f32.s32 	%f1272, %r1334;
	cvt.rn.f32.s32 	%f1273, %r1335;
	cvt.rn.f32.s32 	%f1274, %r1336;
	cvt.rn.f32.s32 	%f1275, %r1337;
	cvt.rn.f32.s32 	%f1276, %r1338;
	cvt.rn.f32.s32 	%f1277, %r1339;
	cvt.rn.f32.s32 	%f1278, %r1340;
	cvt.rn.f32.s32 	%f1279, %r1341;
	cvt.rn.f32.s32 	%f1280, %r1342;
	cvt.rn.f32.s32 	%f1281, %r1343;
	cvt.rn.f32.s32 	%f1282, %r1344;
	cvt.rn.f32.s32 	%f1283, %r1345;
	cvt.rn.f32.s32 	%f1284, %r1346;
	cvt.rn.f32.s32 	%f1285, %r1347;
	cvt.rn.f32.s32 	%f1286, %r1348;
	cvt.rn.f32.s32 	%f1287, %r1349;
	cvt.rn.f32.s32 	%f1288, %r1350;
	cvt.rn.f32.s32 	%f1289, %r1351;
	cvt.rn.f32.s32 	%f1290, %r1352;
	cvt.rn.f32.s32 	%f1291, %r1353;
	cvt.rn.f32.s32 	%f1292, %r1354;
	cvt.rn.f32.s32 	%f1293, %r1355;
	cvt.rn.f32.s32 	%f1294, %r1356;
	cvt.rn.f32.s32 	%f1295, %r1357;
	cvt.rn.f32.s32 	%f1296, %r1358;
	cvt.rn.f32.s32 	%f1297, %r1359;
	cvt.rn.f32.s32 	%f1298, %r1360;
	cvt.rn.f32.s32 	%f1299, %r1361;
	cvt.rn.f32.s32 	%f1300, %r1362;
	cvt.rn.f32.s32 	%f1301, %r1363;
	.loc	1 394 19
	mul.f32 	%f1303, %f1302, %f1254;
	mul.f32 	%f1304, %f1302, %f1255;
	mul.f32 	%f1305, %f1302, %f1256;
	mul.f32 	%f1306, %f1302, %f1257;
	mul.f32 	%f1307, %f1302, %f1258;
	mul.f32 	%f1308, %f1302, %f1259;
	mul.f32 	%f1309, %f1302, %f1260;
	mul.f32 	%f1310, %f1302, %f1261;
	mul.f32 	%f1311, %f1302, %f1262;
	mul.f32 	%f1312, %f1302, %f1263;
	mul.f32 	%f1313, %f1302, %f1264;
	mul.f32 	%f1314, %f1302, %f1265;
	mul.f32 	%f1315, %f1302, %f1266;
	mul.f32 	%f1316, %f1302, %f1267;
	mul.f32 	%f1317, %f1302, %f1268;
	mul.f32 	%f1318, %f1302, %f1269;
	mul.f32 	%f1319, %f1302, %f1270;
	mul.f32 	%f1320, %f1302, %f1271;
	mul.f32 	%f1321, %f1302, %f1272;
	mul.f32 	%f1322, %f1302, %f1273;
	mul.f32 	%f1323, %f1302, %f1274;
	mul.f32 	%f1324, %f1302, %f1275;
	mul.f32 	%f1325, %f1302, %f1276;
	mul.f32 	%f1326, %f1302, %f1277;
	mul.f32 	%f1327, %f1302, %f1278;
	mul.f32 	%f1328, %f1302, %f1279;
	mul.f32 	%f1329, %f1302, %f1280;
	mul.f32 	%f1330, %f1302, %f1281;
	mul.f32 	%f1331, %f1302, %f1282;
	mul.f32 	%f1332, %f1302, %f1283;
	mul.f32 	%f1333, %f1302, %f1284;
	mul.f32 	%f1334, %f1302, %f1285;
	mul.f32 	%f1335, %f1302, %f1286;
	mul.f32 	%f1336, %f1302, %f1287;
	mul.f32 	%f1337, %f1302, %f1288;
	mul.f32 	%f1338, %f1302, %f1289;
	mul.f32 	%f1339, %f1302, %f1290;
	mul.f32 	%f1340, %f1302, %f1291;
	mul.f32 	%f1341, %f1302, %f1292;
	mul.f32 	%f1342, %f1302, %f1293;
	mul.f32 	%f1343, %f1302, %f1294;
	mul.f32 	%f1344, %f1302, %f1295;
	mul.f32 	%f1345, %f1302, %f1296;
	mul.f32 	%f1346, %f1302, %f1297;
	mul.f32 	%f1347, %f1302, %f1298;
	mul.f32 	%f1348, %f1302, %f1299;
	mul.f32 	%f1349, %f1302, %f1300;
	mul.f32 	%f1350, %f1302, %f1301;
	.loc	1 395 19
	fma.rn.f32 	%f1351, %f358, 0f3E000000, %f1303;
	fma.rn.f32 	%f1352, %f359, 0f3E000000, %f1304;
	fma.rn.f32 	%f1353, %f360, 0f3E000000, %f1305;
	fma.rn.f32 	%f1354, %f361, 0f3E000000, %f1306;
	fma.rn.f32 	%f1355, %f366, 0f3E000000, %f1307;
	fma.rn.f32 	%f1356, %f367, 0f3E000000, %f1308;
	fma.rn.f32 	%f1357, %f368, 0f3E000000, %f1309;
	fma.rn.f32 	%f1358, %f369, 0f3E000000, %f1310;
	fma.rn.f32 	%f1359, %f374, 0f3E000000, %f1311;
	fma.rn.f32 	%f1360, %f375, 0f3E000000, %f1312;
	fma.rn.f32 	%f1361, %f376, 0f3E000000, %f1313;
	fma.rn.f32 	%f1362, %f377, 0f3E000000, %f1314;
	fma.rn.f32 	%f1363, %f382, 0f3E000000, %f1315;
	fma.rn.f32 	%f1364, %f383, 0f3E000000, %f1316;
	fma.rn.f32 	%f1365, %f384, 0f3E000000, %f1317;
	fma.rn.f32 	%f1366, %f385, 0f3E000000, %f1318;
	fma.rn.f32 	%f1367, %f390, 0f3E000000, %f1319;
	fma.rn.f32 	%f1368, %f391, 0f3E000000, %f1320;
	fma.rn.f32 	%f1369, %f392, 0f3E000000, %f1321;
	fma.rn.f32 	%f1370, %f393, 0f3E000000, %f1322;
	fma.rn.f32 	%f1371, %f398, 0f3E000000, %f1323;
	fma.rn.f32 	%f1372, %f399, 0f3E000000, %f1324;
	fma.rn.f32 	%f1373, %f400, 0f3E000000, %f1325;
	fma.rn.f32 	%f1374, %f401, 0f3E000000, %f1326;
	fma.rn.f32 	%f1375, %f406, 0f3E000000, %f1327;
	fma.rn.f32 	%f1376, %f407, 0f3E000000, %f1328;
	fma.rn.f32 	%f1377, %f408, 0f3E000000, %f1329;
	fma.rn.f32 	%f1378, %f409, 0f3E000000, %f1330;
	fma.rn.f32 	%f1379, %f414, 0f3E000000, %f1331;
	fma.rn.f32 	%f1380, %f415, 0f3E000000, %f1332;
	fma.rn.f32 	%f1381, %f416, 0f3E000000, %f1333;
	fma.rn.f32 	%f1382, %f417, 0f3E000000, %f1334;
	fma.rn.f32 	%f1383, %f422, 0f3E000000, %f1335;
	fma.rn.f32 	%f1384, %f423, 0f3E000000, %f1336;
	fma.rn.f32 	%f1385, %f424, 0f3E000000, %f1337;
	fma.rn.f32 	%f1386, %f425, 0f3E000000, %f1338;
	fma.rn.f32 	%f1387, %f430, 0f3E000000, %f1339;
	fma.rn.f32 	%f1388, %f431, 0f3E000000, %f1340;
	fma.rn.f32 	%f1389, %f432, 0f3E000000, %f1335;
	fma.rn.f32 	%f1390, %f433, 0f3E000000, %f1336;
	fma.rn.f32 	%f1391, %f438, 0f3E000000, %f1341;
	fma.rn.f32 	%f1392, %f439, 0f3E000000, %f1342;
	fma.rn.f32 	%f1393, %f440, 0f3E000000, %f1339;
	fma.rn.f32 	%f1394, %f441, 0f3E000000, %f1340;
	fma.rn.f32 	%f1395, %f446, 0f3E000000, %f1343;
	fma.rn.f32 	%f1396, %f447, 0f3E000000, %f1344;
	fma.rn.f32 	%f1397, %f448, 0f3E000000, %f1341;
	fma.rn.f32 	%f1398, %f449, 0f3E000000, %f1342;
	fma.rn.f32 	%f1399, %f454, 0f3E000000, %f1345;
	fma.rn.f32 	%f1400, %f455, 0f3E000000, %f1346;
	fma.rn.f32 	%f1401, %f456, 0f3E000000, %f1343;
	fma.rn.f32 	%f1402, %f457, 0f3E000000, %f1344;
	fma.rn.f32 	%f1403, %f462, 0f3E000000, %f1347;
	fma.rn.f32 	%f1404, %f463, 0f3E000000, %f1348;
	fma.rn.f32 	%f1405, %f464, 0f3E000000, %f1345;
	fma.rn.f32 	%f1406, %f465, 0f3E000000, %f1346;
	fma.rn.f32 	%f1407, %f470, 0f3E000000, %f1349;
	fma.rn.f32 	%f1408, %f471, 0f3E000000, %f1350;
	fma.rn.f32 	%f1409, %f472, 0f3E000000, %f1347;
	fma.rn.f32 	%f1410, %f473, 0f3E000000, %f1348;
	fma.rn.f32 	%f1411, %f478, 0f3E000000, %f1305;
	fma.rn.f32 	%f1412, %f479, 0f3E000000, %f1306;
	fma.rn.f32 	%f1413, %f480, 0f3E000000, %f1349;
	fma.rn.f32 	%f1414, %f481, 0f3E000000, %f1350;
	.loc	1 415 27
	mul.f32 	%f1415, %f1351, 0f3FB8AA3B;
	mul.f32 	%f1416, %f1352, 0f3FB8AA3B;
	mul.f32 	%f1417, %f1353, 0f3FB8AA3B;
	mul.f32 	%f1418, %f1354, 0f3FB8AA3B;
	mul.f32 	%f1419, %f1355, 0f3FB8AA3B;
	mul.f32 	%f1420, %f1356, 0f3FB8AA3B;
	mul.f32 	%f1421, %f1357, 0f3FB8AA3B;
	mul.f32 	%f1422, %f1358, 0f3FB8AA3B;
	mul.f32 	%f1423, %f1359, 0f3FB8AA3B;
	mul.f32 	%f1424, %f1360, 0f3FB8AA3B;
	mul.f32 	%f1425, %f1361, 0f3FB8AA3B;
	mul.f32 	%f1426, %f1362, 0f3FB8AA3B;
	mul.f32 	%f1427, %f1363, 0f3FB8AA3B;
	mul.f32 	%f1428, %f1364, 0f3FB8AA3B;
	mul.f32 	%f1429, %f1365, 0f3FB8AA3B;
	mul.f32 	%f1430, %f1366, 0f3FB8AA3B;
	mul.f32 	%f1431, %f1367, 0f3FB8AA3B;
	mul.f32 	%f1432, %f1368, 0f3FB8AA3B;
	mul.f32 	%f1433, %f1369, 0f3FB8AA3B;
	mul.f32 	%f1434, %f1370, 0f3FB8AA3B;
	mul.f32 	%f1435, %f1371, 0f3FB8AA3B;
	mul.f32 	%f1436, %f1372, 0f3FB8AA3B;
	mul.f32 	%f1437, %f1373, 0f3FB8AA3B;
	mul.f32 	%f1438, %f1374, 0f3FB8AA3B;
	mul.f32 	%f1439, %f1375, 0f3FB8AA3B;
	mul.f32 	%f1440, %f1376, 0f3FB8AA3B;
	mul.f32 	%f1441, %f1377, 0f3FB8AA3B;
	mul.f32 	%f1442, %f1378, 0f3FB8AA3B;
	mul.f32 	%f1443, %f1379, 0f3FB8AA3B;
	mul.f32 	%f1444, %f1380, 0f3FB8AA3B;
	mul.f32 	%f1445, %f1381, 0f3FB8AA3B;
	mul.f32 	%f1446, %f1382, 0f3FB8AA3B;
	mul.f32 	%f1447, %f1383, 0f3FB8AA3B;
	mul.f32 	%f1448, %f1384, 0f3FB8AA3B;
	mul.f32 	%f1449, %f1385, 0f3FB8AA3B;
	mul.f32 	%f1450, %f1386, 0f3FB8AA3B;
	mul.f32 	%f1451, %f1387, 0f3FB8AA3B;
	mul.f32 	%f1452, %f1388, 0f3FB8AA3B;
	mul.f32 	%f1453, %f1389, 0f3FB8AA3B;
	mul.f32 	%f1454, %f1390, 0f3FB8AA3B;
	mul.f32 	%f1455, %f1391, 0f3FB8AA3B;
	mul.f32 	%f1456, %f1392, 0f3FB8AA3B;
	mul.f32 	%f1457, %f1393, 0f3FB8AA3B;
	mul.f32 	%f1458, %f1394, 0f3FB8AA3B;
	mul.f32 	%f1459, %f1395, 0f3FB8AA3B;
	mul.f32 	%f1460, %f1396, 0f3FB8AA3B;
	mul.f32 	%f1461, %f1397, 0f3FB8AA3B;
	mul.f32 	%f1462, %f1398, 0f3FB8AA3B;
	mul.f32 	%f1463, %f1399, 0f3FB8AA3B;
	mul.f32 	%f1464, %f1400, 0f3FB8AA3B;
	mul.f32 	%f1465, %f1401, 0f3FB8AA3B;
	mul.f32 	%f1466, %f1402, 0f3FB8AA3B;
	mul.f32 	%f1467, %f1403, 0f3FB8AA3B;
	mul.f32 	%f1468, %f1404, 0f3FB8AA3B;
	mul.f32 	%f1469, %f1405, 0f3FB8AA3B;
	mul.f32 	%f1470, %f1406, 0f3FB8AA3B;
	mul.f32 	%f1471, %f1407, 0f3FB8AA3B;
	mul.f32 	%f1472, %f1408, 0f3FB8AA3B;
	mul.f32 	%f1473, %f1409, 0f3FB8AA3B;
	mul.f32 	%f1474, %f1410, 0f3FB8AA3B;
	mul.f32 	%f1475, %f1411, 0f3FB8AA3B;
	mul.f32 	%f1476, %f1412, 0f3FB8AA3B;
	mul.f32 	%f1477, %f1413, 0f3FB8AA3B;
	mul.f32 	%f1478, %f1414, 0f3FB8AA3B;
	.loc	2 163 27
	max.f32 	%f1479, %f1415, %f1416;
	max.f32 	%f1480, %f1417, %f1418;
	max.f32 	%f1481, %f1479, %f1419;
	max.f32 	%f1482, %f1481, %f1420;
	max.f32 	%f1483, %f1480, %f1421;
	max.f32 	%f1484, %f1483, %f1422;
	max.f32 	%f1485, %f1482, %f1423;
	max.f32 	%f1486, %f1485, %f1424;
	max.f32 	%f1487, %f1484, %f1425;
	max.f32 	%f1488, %f1487, %f1426;
	max.f32 	%f1489, %f1486, %f1427;
	max.f32 	%f1490, %f1489, %f1428;
	max.f32 	%f1491, %f1488, %f1429;
	max.f32 	%f1492, %f1491, %f1430;
	max.f32 	%f1493, %f1490, %f1431;
	max.f32 	%f1494, %f1493, %f1432;
	max.f32 	%f1495, %f1492, %f1433;
	max.f32 	%f1496, %f1495, %f1434;
	max.f32 	%f1497, %f1494, %f1435;
	max.f32 	%f1498, %f1497, %f1436;
	max.f32 	%f1499, %f1496, %f1437;
	max.f32 	%f1500, %f1499, %f1438;
	max.f32 	%f1501, %f1498, %f1439;
	max.f32 	%f1502, %f1501, %f1440;
	max.f32 	%f1503, %f1500, %f1441;
	max.f32 	%f1504, %f1503, %f1442;
	max.f32 	%f1505, %f1502, %f1443;
	max.f32 	%f1506, %f1505, %f1444;
	max.f32 	%f1507, %f1504, %f1445;
	max.f32 	%f1508, %f1507, %f1446;
	max.f32 	%f1509, %f1447, %f1448;
	max.f32 	%f1510, %f1449, %f1450;
	max.f32 	%f1511, %f1509, %f1451;
	max.f32 	%f1512, %f1511, %f1452;
	max.f32 	%f1513, %f1510, %f1453;
	max.f32 	%f1514, %f1513, %f1454;
	max.f32 	%f1515, %f1512, %f1455;
	max.f32 	%f1516, %f1515, %f1456;
	max.f32 	%f1517, %f1514, %f1457;
	max.f32 	%f1518, %f1517, %f1458;
	max.f32 	%f1519, %f1516, %f1459;
	max.f32 	%f1520, %f1519, %f1460;
	max.f32 	%f1521, %f1518, %f1461;
	max.f32 	%f1522, %f1521, %f1462;
	max.f32 	%f1523, %f1520, %f1463;
	max.f32 	%f1524, %f1523, %f1464;
	max.f32 	%f1525, %f1522, %f1465;
	max.f32 	%f1526, %f1525, %f1466;
	max.f32 	%f1527, %f1524, %f1467;
	max.f32 	%f1528, %f1527, %f1468;
	max.f32 	%f1529, %f1526, %f1469;
	max.f32 	%f1530, %f1529, %f1470;
	max.f32 	%f1531, %f1528, %f1471;
	max.f32 	%f1532, %f1531, %f1472;
	max.f32 	%f1533, %f1530, %f1473;
	max.f32 	%f1534, %f1533, %f1474;
	max.f32 	%f1535, %f1532, %f1475;
	max.f32 	%f1536, %f1535, %f1476;
	max.f32 	%f1537, %f1534, %f1477;
	max.f32 	%f1538, %f1537, %f1478;
	.loc	2 184 40
	mov.b32 	%r1364, %f1506;
	shfl.sync.bfly.b32	%r1365, %r1364, 2, 31, -1;
	mov.b32 	%f1539, %r1365;
	mov.b32 	%r1366, %f1508;
	mov.b32 	%r1367, %f1536;
	mov.b32 	%r1368, %f1538;
	.loc	2 163 27
	max.f32 	%f1540, %f1506, %f1539;
	.loc	2 184 40
	mov.b32 	%r1369, %f1540;
	shfl.sync.bfly.b32	%r1370, %r1369, 1, 31, -1;
	shfl.sync.bfly.b32	%r1371, %r1366, 2, 31, -1;
	mov.b32 	%f1541, %r1371;
	.loc	2 163 27
	max.f32 	%f1542, %f1508, %f1541;
	.loc	2 184 40
	mov.b32 	%r1372, %f1542;
	shfl.sync.bfly.b32	%r1373, %r1372, 1, 31, -1;
	shfl.sync.bfly.b32	%r1374, %r1367, 2, 31, -1;
	mov.b32 	%f1543, %r1374;
	.loc	2 163 27
	max.f32 	%f1544, %f1536, %f1543;
	.loc	2 184 40
	mov.b32 	%r1375, %f1544;
	shfl.sync.bfly.b32	%r1376, %r1375, 1, 31, -1;
	shfl.sync.bfly.b32	%r1377, %r1368, 2, 31, -1;
	mov.b32 	%f1545, %r1377;
	.loc	2 163 27
	max.f32 	%f1546, %f1538, %f1545;
	.loc	2 184 40
	mov.b32 	%r1378, %f1546;
	shfl.sync.bfly.b32	%r1379, %r1378, 1, 31, -1;
	mov.b32 	%f1547, %r1379;
	mov.b32 	%f1548, %r1376;
	mov.b32 	%f1549, %r1373;
	mov.b32 	%f1550, %r1370;
	.loc	2 163 27
	max.f32 	%f1551, %f1540, %f1550;
	max.f32 	%f1552, %f1542, %f1549;
	max.f32 	%f1553, %f1544, %f1548;
	max.f32 	%f1554, %f1546, %f1547;
	.loc	1 419 27
	max.f32 	%f77, %f1850, %f1554;
	max.f32 	%f76, %f1849, %f1553;
	max.f32 	%f75, %f1848, %f1552;
	max.f32 	%f74, %f1847, %f1551;
	.loc	1 421 35
	setp.eq.f32 	%p41, %f74, 0fFF800000;
	setp.eq.f32 	%p42, %f75, 0fFF800000;
	setp.eq.f32 	%p43, %f76, 0fFF800000;
	setp.eq.f32 	%p44, %f77, 0fFF800000;
	.loc	1 422 51
	selp.f32 	%f1555, 0f00000000, %f74, %p41;
	selp.f32 	%f1556, 0f00000000, %f75, %p42;
	selp.f32 	%f1557, 0f00000000, %f76, %p43;
	selp.f32 	%f1558, 0f00000000, %f77, %p44;
	.loc	1 426 31
	sub.f32 	%f1559, %f1847, %f1555;
	sub.f32 	%f1560, %f1848, %f1556;
	sub.f32 	%f1561, %f1849, %f1557;
	sub.f32 	%f1562, %f1850, %f1558;
	.loc	1 426 25
	ex2.approx.ftz.f32 	%f1563, %f1559;
	ex2.approx.ftz.f32 	%f1564, %f1560;
	ex2.approx.ftz.f32 	%f1565, %f1561;
	ex2.approx.ftz.f32 	%f1566, %f1562;
	.loc	1 427 39
	neg.f32 	%f1567, %f1555;
	fma.rn.f32 	%f1568, %f1351, 0f3FB8AA3B, %f1567;
	fma.rn.f32 	%f1569, %f1352, 0f3FB8AA3B, %f1567;
	neg.f32 	%f1570, %f1556;
	fma.rn.f32 	%f1571, %f1353, 0f3FB8AA3B, %f1570;
	fma.rn.f32 	%f1572, %f1354, 0f3FB8AA3B, %f1570;
	fma.rn.f32 	%f1573, %f1355, 0f3FB8AA3B, %f1567;
	fma.rn.f32 	%f1574, %f1356, 0f3FB8AA3B, %f1567;
	fma.rn.f32 	%f1575, %f1357, 0f3FB8AA3B, %f1570;
	fma.rn.f32 	%f1576, %f1358, 0f3FB8AA3B, %f1570;
	fma.rn.f32 	%f1577, %f1359, 0f3FB8AA3B, %f1567;
	fma.rn.f32 	%f1578, %f1360, 0f3FB8AA3B, %f1567;
	fma.rn.f32 	%f1579, %f1361, 0f3FB8AA3B, %f1570;
	fma.rn.f32 	%f1580, %f1362, 0f3FB8AA3B, %f1570;
	fma.rn.f32 	%f1581, %f1363, 0f3FB8AA3B, %f1567;
	fma.rn.f32 	%f1582, %f1364, 0f3FB8AA3B, %f1567;
	fma.rn.f32 	%f1583, %f1365, 0f3FB8AA3B, %f1570;
	fma.rn.f32 	%f1584, %f1366, 0f3FB8AA3B, %f1570;
	fma.rn.f32 	%f1585, %f1367, 0f3FB8AA3B, %f1567;
	fma.rn.f32 	%f1586, %f1368, 0f3FB8AA3B, %f1567;
	fma.rn.f32 	%f1587, %f1369, 0f3FB8AA3B, %f1570;
	fma.rn.f32 	%f1588, %f1370, 0f3FB8AA3B, %f1570;
	fma.rn.f32 	%f1589, %f1371, 0f3FB8AA3B, %f1567;
	fma.rn.f32 	%f1590, %f1372, 0f3FB8AA3B, %f1567;
	fma.rn.f32 	%f1591, %f1373, 0f3FB8AA3B, %f1570;
	fma.rn.f32 	%f1592, %f1374, 0f3FB8AA3B, %f1570;
	fma.rn.f32 	%f1593, %f1375, 0f3FB8AA3B, %f1567;
	fma.rn.f32 	%f1594, %f1376, 0f3FB8AA3B, %f1567;
	fma.rn.f32 	%f1595, %f1377, 0f3FB8AA3B, %f1570;
	fma.rn.f32 	%f1596, %f1378, 0f3FB8AA3B, %f1570;
	fma.rn.f32 	%f1597, %f1379, 0f3FB8AA3B, %f1567;
	fma.rn.f32 	%f1598, %f1380, 0f3FB8AA3B, %f1567;
	fma.rn.f32 	%f1599, %f1381, 0f3FB8AA3B, %f1570;
	fma.rn.f32 	%f1600, %f1382, 0f3FB8AA3B, %f1570;
	neg.f32 	%f1601, %f1557;
	fma.rn.f32 	%f1602, %f1383, 0f3FB8AA3B, %f1601;
	fma.rn.f32 	%f1603, %f1384, 0f3FB8AA3B, %f1601;
	neg.f32 	%f1604, %f1558;
	fma.rn.f32 	%f1605, %f1385, 0f3FB8AA3B, %f1604;
	fma.rn.f32 	%f1606, %f1386, 0f3FB8AA3B, %f1604;
	fma.rn.f32 	%f1607, %f1387, 0f3FB8AA3B, %f1601;
	fma.rn.f32 	%f1608, %f1388, 0f3FB8AA3B, %f1601;
	fma.rn.f32 	%f1609, %f1389, 0f3FB8AA3B, %f1604;
	fma.rn.f32 	%f1610, %f1390, 0f3FB8AA3B, %f1604;
	fma.rn.f32 	%f1611, %f1391, 0f3FB8AA3B, %f1601;
	fma.rn.f32 	%f1612, %f1392, 0f3FB8AA3B, %f1601;
	fma.rn.f32 	%f1613, %f1393, 0f3FB8AA3B, %f1604;
	fma.rn.f32 	%f1614, %f1394, 0f3FB8AA3B, %f1604;
	fma.rn.f32 	%f1615, %f1395, 0f3FB8AA3B, %f1601;
	fma.rn.f32 	%f1616, %f1396, 0f3FB8AA3B, %f1601;
	fma.rn.f32 	%f1617, %f1397, 0f3FB8AA3B, %f1604;
	fma.rn.f32 	%f1618, %f1398, 0f3FB8AA3B, %f1604;
	fma.rn.f32 	%f1619, %f1399, 0f3FB8AA3B, %f1601;
	fma.rn.f32 	%f1620, %f1400, 0f3FB8AA3B, %f1601;
	fma.rn.f32 	%f1621, %f1401, 0f3FB8AA3B, %f1604;
	fma.rn.f32 	%f1622, %f1402, 0f3FB8AA3B, %f1604;
	fma.rn.f32 	%f1623, %f1403, 0f3FB8AA3B, %f1601;
	fma.rn.f32 	%f1624, %f1404, 0f3FB8AA3B, %f1601;
	fma.rn.f32 	%f1625, %f1405, 0f3FB8AA3B, %f1604;
	fma.rn.f32 	%f1626, %f1406, 0f3FB8AA3B, %f1604;
	fma.rn.f32 	%f1627, %f1407, 0f3FB8AA3B, %f1601;
	fma.rn.f32 	%f1628, %f1408, 0f3FB8AA3B, %f1601;
	fma.rn.f32 	%f1629, %f1409, 0f3FB8AA3B, %f1604;
	fma.rn.f32 	%f1630, %f1410, 0f3FB8AA3B, %f1604;
	fma.rn.f32 	%f1631, %f1411, 0f3FB8AA3B, %f1601;
	fma.rn.f32 	%f1632, %f1412, 0f3FB8AA3B, %f1601;
	fma.rn.f32 	%f1633, %f1413, 0f3FB8AA3B, %f1604;
	fma.rn.f32 	%f1634, %f1414, 0f3FB8AA3B, %f1604;
	.loc	1 427 21
	ex2.approx.ftz.f32 	%f1635, %f1568;
	ex2.approx.ftz.f32 	%f1636, %f1569;
	ex2.approx.ftz.f32 	%f1637, %f1571;
	ex2.approx.ftz.f32 	%f1638, %f1572;
	ex2.approx.ftz.f32 	%f1639, %f1573;
	ex2.approx.ftz.f32 	%f1640, %f1574;
	ex2.approx.ftz.f32 	%f1641, %f1575;
	ex2.approx.ftz.f32 	%f1642, %f1576;
	ex2.approx.ftz.f32 	%f1643, %f1577;
	ex2.approx.ftz.f32 	%f1644, %f1578;
	ex2.approx.ftz.f32 	%f1645, %f1579;
	ex2.approx.ftz.f32 	%f1646, %f1580;
	ex2.approx.ftz.f32 	%f1647, %f1581;
	ex2.approx.ftz.f32 	%f1648, %f1582;
	ex2.approx.ftz.f32 	%f1649, %f1583;
	ex2.approx.ftz.f32 	%f1650, %f1584;
	ex2.approx.ftz.f32 	%f1651, %f1585;
	ex2.approx.ftz.f32 	%f1652, %f1586;
	ex2.approx.ftz.f32 	%f1653, %f1587;
	ex2.approx.ftz.f32 	%f1654, %f1588;
	ex2.approx.ftz.f32 	%f1655, %f1589;
	ex2.approx.ftz.f32 	%f1656, %f1590;
	ex2.approx.ftz.f32 	%f1657, %f1591;
	ex2.approx.ftz.f32 	%f1658, %f1592;
	ex2.approx.ftz.f32 	%f1659, %f1593;
	ex2.approx.ftz.f32 	%f1660, %f1594;
	ex2.approx.ftz.f32 	%f1661, %f1595;
	ex2.approx.ftz.f32 	%f1662, %f1596;
	ex2.approx.ftz.f32 	%f1663, %f1597;
	ex2.approx.ftz.f32 	%f1664, %f1598;
	ex2.approx.ftz.f32 	%f1665, %f1599;
	ex2.approx.ftz.f32 	%f1666, %f1600;
	ex2.approx.ftz.f32 	%f1667, %f1602;
	ex2.approx.ftz.f32 	%f1668, %f1603;
	ex2.approx.ftz.f32 	%f1669, %f1605;
	ex2.approx.ftz.f32 	%f1670, %f1606;
	ex2.approx.ftz.f32 	%f1671, %f1607;
	ex2.approx.ftz.f32 	%f1672, %f1608;
	ex2.approx.ftz.f32 	%f1673, %f1609;
	ex2.approx.ftz.f32 	%f1674, %f1610;
	ex2.approx.ftz.f32 	%f1675, %f1611;
	ex2.approx.ftz.f32 	%f1676, %f1612;
	ex2.approx.ftz.f32 	%f1677, %f1613;
	ex2.approx.ftz.f32 	%f1678, %f1614;
	ex2.approx.ftz.f32 	%f1679, %f1615;
	ex2.approx.ftz.f32 	%f1680, %f1616;
	ex2.approx.ftz.f32 	%f1681, %f1617;
	ex2.approx.ftz.f32 	%f1682, %f1618;
	ex2.approx.ftz.f32 	%f1683, %f1619;
	ex2.approx.ftz.f32 	%f1684, %f1620;
	ex2.approx.ftz.f32 	%f1685, %f1621;
	ex2.approx.ftz.f32 	%f1686, %f1622;
	ex2.approx.ftz.f32 	%f1687, %f1623;
	ex2.approx.ftz.f32 	%f1688, %f1624;
	ex2.approx.ftz.f32 	%f1689, %f1625;
	ex2.approx.ftz.f32 	%f1690, %f1626;
	ex2.approx.ftz.f32 	%f1691, %f1627;
	ex2.approx.ftz.f32 	%f1692, %f1628;
	ex2.approx.ftz.f32 	%f1693, %f1629;
	ex2.approx.ftz.f32 	%f1694, %f1630;
	ex2.approx.ftz.f32 	%f1695, %f1631;
	ex2.approx.ftz.f32 	%f1696, %f1632;
	ex2.approx.ftz.f32 	%f1697, %f1633;
	ex2.approx.ftz.f32 	%f1698, %f1634;
	.loc	2 256 15
	add.f32 	%f1699, %f1635, %f1636;
	add.f32 	%f1700, %f1637, %f1638;
	add.f32 	%f1701, %f1699, %f1639;
	add.f32 	%f1702, %f1701, %f1640;
	add.f32 	%f1703, %f1700, %f1641;
	add.f32 	%f1704, %f1703, %f1642;
	add.f32 	%f1705, %f1702, %f1643;
	add.f32 	%f1706, %f1705, %f1644;
	add.f32 	%f1707, %f1704, %f1645;
	add.f32 	%f1708, %f1707, %f1646;
	add.f32 	%f1709, %f1706, %f1647;
	add.f32 	%f1710, %f1709, %f1648;
	add.f32 	%f1711, %f1708, %f1649;
	add.f32 	%f1712, %f1711, %f1650;
	add.f32 	%f1713, %f1710, %f1651;
	add.f32 	%f1714, %f1713, %f1652;
	add.f32 	%f1715, %f1712, %f1653;
	add.f32 	%f1716, %f1715, %f1654;
	add.f32 	%f1717, %f1714, %f1655;
	add.f32 	%f1718, %f1717, %f1656;
	add.f32 	%f1719, %f1716, %f1657;
	add.f32 	%f1720, %f1719, %f1658;
	add.f32 	%f1721, %f1718, %f1659;
	add.f32 	%f1722, %f1721, %f1660;
	add.f32 	%f1723, %f1720, %f1661;
	add.f32 	%f1724, %f1723, %f1662;
	add.f32 	%f1725, %f1722, %f1663;
	add.f32 	%f1726, %f1725, %f1664;
	add.f32 	%f1727, %f1724, %f1665;
	add.f32 	%f1728, %f1727, %f1666;
	add.f32 	%f1729, %f1667, %f1668;
	add.f32 	%f1730, %f1669, %f1670;
	add.f32 	%f1731, %f1729, %f1671;
	add.f32 	%f1732, %f1731, %f1672;
	add.f32 	%f1733, %f1730, %f1673;
	add.f32 	%f1734, %f1733, %f1674;
	add.f32 	%f1735, %f1732, %f1675;
	add.f32 	%f1736, %f1735, %f1676;
	add.f32 	%f1737, %f1734, %f1677;
	add.f32 	%f1738, %f1737, %f1678;
	add.f32 	%f1739, %f1736, %f1679;
	add.f32 	%f1740, %f1739, %f1680;
	add.f32 	%f1741, %f1738, %f1681;
	add.f32 	%f1742, %f1741, %f1682;
	add.f32 	%f1743, %f1740, %f1683;
	add.f32 	%f1744, %f1743, %f1684;
	add.f32 	%f1745, %f1742, %f1685;
	add.f32 	%f1746, %f1745, %f1686;
	add.f32 	%f1747, %f1744, %f1687;
	add.f32 	%f1748, %f1747, %f1688;
	add.f32 	%f1749, %f1746, %f1689;
	add.f32 	%f1750, %f1749, %f1690;
	add.f32 	%f1751, %f1748, %f1691;
	add.f32 	%f1752, %f1751, %f1692;
	add.f32 	%f1753, %f1750, %f1693;
	add.f32 	%f1754, %f1753, %f1694;
	add.f32 	%f1755, %f1752, %f1695;
	add.f32 	%f1756, %f1755, %f1696;
	add.f32 	%f1757, %f1754, %f1697;
	add.f32 	%f1758, %f1757, %f1698;
	.loc	2 267 36
	mov.b32 	%r1380, %f1726;
	shfl.sync.bfly.b32	%r1381, %r1380, 2, 31, -1;
	mov.b32 	%f1759, %r1381;
	mov.b32 	%r1382, %f1728;
	mov.b32 	%r1383, %f1756;
	mov.b32 	%r1384, %f1758;
	.loc	2 256 15
	add.f32 	%f1760, %f1726, %f1759;
	.loc	2 267 36
	mov.b32 	%r1385, %f1760;
	shfl.sync.bfly.b32	%r1386, %r1385, 1, 31, -1;
	shfl.sync.bfly.b32	%r1387, %r1382, 2, 31, -1;
	mov.b32 	%f1761, %r1387;
	.loc	2 256 15
	add.f32 	%f1762, %f1728, %f1761;
	.loc	2 267 36
	mov.b32 	%r1388, %f1762;
	shfl.sync.bfly.b32	%r1389, %r1388, 1, 31, -1;
	shfl.sync.bfly.b32	%r1390, %r1383, 2, 31, -1;
	mov.b32 	%f1763, %r1390;
	.loc	2 256 15
	add.f32 	%f1764, %f1756, %f1763;
	.loc	2 267 36
	mov.b32 	%r1391, %f1764;
	shfl.sync.bfly.b32	%r1392, %r1391, 1, 31, -1;
	shfl.sync.bfly.b32	%r1393, %r1384, 2, 31, -1;
	mov.b32 	%f1765, %r1393;
	.loc	2 256 15
	add.f32 	%f1766, %f1758, %f1765;
	.loc	2 267 36
	mov.b32 	%r1394, %f1766;
	shfl.sync.bfly.b32	%r1395, %r1394, 1, 31, -1;
	mov.b32 	%f1767, %r1386;
	mov.b32 	%f1768, %r1389;
	mov.b32 	%f1769, %r1392;
	mov.b32 	%f1770, %r1395;
	.loc	2 256 15
	add.f32 	%f1771, %f1766, %f1770;
	add.f32 	%f1772, %f1764, %f1769;
	add.f32 	%f1773, %f1762, %f1768;
	add.f32 	%f1774, %f1760, %f1767;
	.loc	1 432 24
	fma.rn.f32 	%f1915, %f1915, %f1563, %f1774;
	fma.rn.f32 	%f1916, %f1916, %f1564, %f1773;
	fma.rn.f32 	%f1917, %f1917, %f1565, %f1772;
	fma.rn.f32 	%f1918, %f1918, %f1566, %f1771;
	.loc	1 434 16
	mul.f32 	%f870, %f870, %f1563;
	mul.f32 	%f871, %f871, %f1563;
	mul.f32 	%f872, %f872, %f1564;
	mul.f32 	%f873, %f873, %f1564;
	mul.f32 	%f878, %f878, %f1563;
	mul.f32 	%f879, %f879, %f1563;
	mul.f32 	%f880, %f880, %f1564;
	mul.f32 	%f881, %f881, %f1564;
	mul.f32 	%f886, %f886, %f1563;
	mul.f32 	%f887, %f887, %f1563;
	mul.f32 	%f888, %f888, %f1564;
	mul.f32 	%f889, %f889, %f1564;
	mul.f32 	%f894, %f894, %f1563;
	mul.f32 	%f895, %f895, %f1563;
	mul.f32 	%f896, %f896, %f1564;
	mul.f32 	%f897, %f897, %f1564;
	mul.f32 	%f902, %f902, %f1563;
	mul.f32 	%f903, %f903, %f1563;
	mul.f32 	%f904, %f904, %f1564;
	mul.f32 	%f905, %f905, %f1564;
	mul.f32 	%f910, %f910, %f1563;
	mul.f32 	%f911, %f911, %f1563;
	mul.f32 	%f912, %f912, %f1564;
	mul.f32 	%f913, %f913, %f1564;
	mul.f32 	%f918, %f918, %f1563;
	mul.f32 	%f919, %f919, %f1563;
	mul.f32 	%f920, %f920, %f1564;
	mul.f32 	%f921, %f921, %f1564;
	mul.f32 	%f926, %f926, %f1563;
	mul.f32 	%f927, %f927, %f1563;
	mul.f32 	%f928, %f928, %f1564;
	mul.f32 	%f929, %f929, %f1564;
	mul.f32 	%f934, %f934, %f1565;
	mul.f32 	%f935, %f935, %f1565;
	mul.f32 	%f936, %f936, %f1566;
	mul.f32 	%f937, %f937, %f1566;
	mul.f32 	%f942, %f942, %f1565;
	mul.f32 	%f943, %f943, %f1565;
	mul.f32 	%f944, %f944, %f1566;
	mul.f32 	%f945, %f945, %f1566;
	mul.f32 	%f950, %f950, %f1565;
	mul.f32 	%f951, %f951, %f1565;
	mul.f32 	%f952, %f952, %f1566;
	mul.f32 	%f953, %f953, %f1566;
	mul.f32 	%f958, %f958, %f1565;
	mul.f32 	%f959, %f959, %f1565;
	mul.f32 	%f960, %f960, %f1566;
	mul.f32 	%f961, %f961, %f1566;
	mul.f32 	%f966, %f966, %f1565;
	mul.f32 	%f967, %f967, %f1565;
	mul.f32 	%f968, %f968, %f1566;
	mul.f32 	%f969, %f969, %f1566;
	mul.f32 	%f974, %f974, %f1565;
	mul.f32 	%f975, %f975, %f1565;
	mul.f32 	%f976, %f976, %f1566;
	mul.f32 	%f977, %f977, %f1566;
	mul.f32 	%f982, %f982, %f1565;
	mul.f32 	%f983, %f983, %f1565;
	mul.f32 	%f984, %f984, %f1566;
	mul.f32 	%f985, %f985, %f1566;
	mul.f32 	%f990, %f990, %f1565;
	mul.f32 	%f991, %f991, %f1565;
	mul.f32 	%f992, %f992, %f1566;
	mul.f32 	%f993, %f993, %f1566;
	.loc	1 440 22
	mov.b32 	%r766, %f1635;
	// begin inline asm
	cvt.rn.bf16.f32 %rs1, %r766;
	// end inline asm
	mov.b32 	%r767, %f1636;
	// begin inline asm
	cvt.rn.bf16.f32 %rs2, %r767;
	// end inline asm
	mov.b32 	%r768, %f1637;
	// begin inline asm
	cvt.rn.bf16.f32 %rs3, %r768;
	// end inline asm
	mov.b32 	%r769, %f1638;
	// begin inline asm
	cvt.rn.bf16.f32 %rs4, %r769;
	// end inline asm
	mov.b32 	%r770, %f1639;
	// begin inline asm
	cvt.rn.bf16.f32 %rs5, %r770;
	// end inline asm
	mov.b32 	%r771, %f1640;
	// begin inline asm
	cvt.rn.bf16.f32 %rs6, %r771;
	// end inline asm
	mov.b32 	%r772, %f1641;
	// begin inline asm
	cvt.rn.bf16.f32 %rs7, %r772;
	// end inline asm
	mov.b32 	%r773, %f1642;
	// begin inline asm
	cvt.rn.bf16.f32 %rs8, %r773;
	// end inline asm
	mov.b32 	%r774, %f1643;
	// begin inline asm
	cvt.rn.bf16.f32 %rs9, %r774;
	// end inline asm
	mov.b32 	%r775, %f1644;
	// begin inline asm
	cvt.rn.bf16.f32 %rs10, %r775;
	// end inline asm
	mov.b32 	%r776, %f1645;
	// begin inline asm
	cvt.rn.bf16.f32 %rs11, %r776;
	// end inline asm
	mov.b32 	%r777, %f1646;
	// begin inline asm
	cvt.rn.bf16.f32 %rs12, %r777;
	// end inline asm
	mov.b32 	%r778, %f1647;
	// begin inline asm
	cvt.rn.bf16.f32 %rs13, %r778;
	// end inline asm
	mov.b32 	%r779, %f1648;
	// begin inline asm
	cvt.rn.bf16.f32 %rs14, %r779;
	// end inline asm
	mov.b32 	%r780, %f1649;
	// begin inline asm
	cvt.rn.bf16.f32 %rs15, %r780;
	// end inline asm
	mov.b32 	%r781, %f1650;
	// begin inline asm
	cvt.rn.bf16.f32 %rs16, %r781;
	// end inline asm
	mov.b32 	%r782, %f1651;
	// begin inline asm
	cvt.rn.bf16.f32 %rs17, %r782;
	// end inline asm
	mov.b32 	%r783, %f1652;
	// begin inline asm
	cvt.rn.bf16.f32 %rs18, %r783;
	// end inline asm
	mov.b32 	%r784, %f1653;
	// begin inline asm
	cvt.rn.bf16.f32 %rs19, %r784;
	// end inline asm
	mov.b32 	%r785, %f1654;
	// begin inline asm
	cvt.rn.bf16.f32 %rs20, %r785;
	// end inline asm
	mov.b32 	%r786, %f1655;
	// begin inline asm
	cvt.rn.bf16.f32 %rs21, %r786;
	// end inline asm
	mov.b32 	%r787, %f1656;
	// begin inline asm
	cvt.rn.bf16.f32 %rs22, %r787;
	// end inline asm
	mov.b32 	%r788, %f1657;
	// begin inline asm
	cvt.rn.bf16.f32 %rs23, %r788;
	// end inline asm
	mov.b32 	%r789, %f1658;
	// begin inline asm
	cvt.rn.bf16.f32 %rs24, %r789;
	// end inline asm
	mov.b32 	%r790, %f1659;
	// begin inline asm
	cvt.rn.bf16.f32 %rs25, %r790;
	// end inline asm
	mov.b32 	%r791, %f1660;
	// begin inline asm
	cvt.rn.bf16.f32 %rs26, %r791;
	// end inline asm
	mov.b32 	%r792, %f1661;
	// begin inline asm
	cvt.rn.bf16.f32 %rs27, %r792;
	// end inline asm
	mov.b32 	%r793, %f1662;
	// begin inline asm
	cvt.rn.bf16.f32 %rs28, %r793;
	// end inline asm
	mov.b32 	%r794, %f1663;
	// begin inline asm
	cvt.rn.bf16.f32 %rs29, %r794;
	// end inline asm
	mov.b32 	%r795, %f1664;
	// begin inline asm
	cvt.rn.bf16.f32 %rs30, %r795;
	// end inline asm
	mov.b32 	%r796, %f1665;
	// begin inline asm
	cvt.rn.bf16.f32 %rs31, %r796;
	// end inline asm
	mov.b32 	%r797, %f1666;
	// begin inline asm
	cvt.rn.bf16.f32 %rs32, %r797;
	// end inline asm
	mov.b32 	%r798, %f1667;
	// begin inline asm
	cvt.rn.bf16.f32 %rs33, %r798;
	// end inline asm
	mov.b32 	%r799, %f1668;
	// begin inline asm
	cvt.rn.bf16.f32 %rs34, %r799;
	// end inline asm
	mov.b32 	%r800, %f1669;
	// begin inline asm
	cvt.rn.bf16.f32 %rs35, %r800;
	// end inline asm
	mov.b32 	%r801, %f1670;
	// begin inline asm
	cvt.rn.bf16.f32 %rs36, %r801;
	// end inline asm
	mov.b32 	%r802, %f1671;
	// begin inline asm
	cvt.rn.bf16.f32 %rs37, %r802;
	// end inline asm
	mov.b32 	%r803, %f1672;
	// begin inline asm
	cvt.rn.bf16.f32 %rs38, %r803;
	// end inline asm
	mov.b32 	%r804, %f1673;
	// begin inline asm
	cvt.rn.bf16.f32 %rs39, %r804;
	// end inline asm
	mov.b32 	%r805, %f1674;
	// begin inline asm
	cvt.rn.bf16.f32 %rs40, %r805;
	// end inline asm
	mov.b32 	%r806, %f1675;
	// begin inline asm
	cvt.rn.bf16.f32 %rs41, %r806;
	// end inline asm
	mov.b32 	%r807, %f1676;
	// begin inline asm
	cvt.rn.bf16.f32 %rs42, %r807;
	// end inline asm
	mov.b32 	%r808, %f1677;
	// begin inline asm
	cvt.rn.bf16.f32 %rs43, %r808;
	// end inline asm
	mov.b32 	%r809, %f1678;
	// begin inline asm
	cvt.rn.bf16.f32 %rs44, %r809;
	// end inline asm
	mov.b32 	%r810, %f1679;
	// begin inline asm
	cvt.rn.bf16.f32 %rs45, %r810;
	// end inline asm
	mov.b32 	%r811, %f1680;
	// begin inline asm
	cvt.rn.bf16.f32 %rs46, %r811;
	// end inline asm
	mov.b32 	%r812, %f1681;
	// begin inline asm
	cvt.rn.bf16.f32 %rs47, %r812;
	// end inline asm
	mov.b32 	%r813, %f1682;
	// begin inline asm
	cvt.rn.bf16.f32 %rs48, %r813;
	// end inline asm
	mov.b32 	%r814, %f1683;
	// begin inline asm
	cvt.rn.bf16.f32 %rs49, %r814;
	// end inline asm
	mov.b32 	%r815, %f1684;
	// begin inline asm
	cvt.rn.bf16.f32 %rs50, %r815;
	// end inline asm
	mov.b32 	%r816, %f1685;
	// begin inline asm
	cvt.rn.bf16.f32 %rs51, %r816;
	// end inline asm
	mov.b32 	%r817, %f1686;
	// begin inline asm
	cvt.rn.bf16.f32 %rs52, %r817;
	// end inline asm
	mov.b32 	%r818, %f1687;
	// begin inline asm
	cvt.rn.bf16.f32 %rs53, %r818;
	// end inline asm
	mov.b32 	%r819, %f1688;
	// begin inline asm
	cvt.rn.bf16.f32 %rs54, %r819;
	// end inline asm
	mov.b32 	%r820, %f1689;
	// begin inline asm
	cvt.rn.bf16.f32 %rs55, %r820;
	// end inline asm
	mov.b32 	%r821, %f1690;
	// begin inline asm
	cvt.rn.bf16.f32 %rs56, %r821;
	// end inline asm
	mov.b32 	%r822, %f1691;
	// begin inline asm
	cvt.rn.bf16.f32 %rs57, %r822;
	// end inline asm
	mov.b32 	%r823, %f1692;
	// begin inline asm
	cvt.rn.bf16.f32 %rs58, %r823;
	// end inline asm
	mov.b32 	%r824, %f1693;
	// begin inline asm
	cvt.rn.bf16.f32 %rs59, %r824;
	// end inline asm
	mov.b32 	%r825, %f1694;
	// begin inline asm
	cvt.rn.bf16.f32 %rs60, %r825;
	// end inline asm
	mov.b32 	%r826, %f1695;
	// begin inline asm
	cvt.rn.bf16.f32 %rs61, %r826;
	// end inline asm
	mov.b32 	%r827, %f1696;
	// begin inline asm
	cvt.rn.bf16.f32 %rs62, %r827;
	// end inline asm
	mov.b32 	%r828, %f1697;
	// begin inline asm
	cvt.rn.bf16.f32 %rs63, %r828;
	// end inline asm
	mov.b32 	%r829, %f1698;
	// begin inline asm
	cvt.rn.bf16.f32 %rs64, %r829;
	// end inline asm
	cvt.u32.u16 	%r1396, %rs1;
	cvt.u32.u16 	%r1397, %rs2;
	shl.b32 	%r1398, %r1397, 16;
	or.b32  	%r910, %r1398, %r1396;
	cvt.u32.u16 	%r1399, %rs3;
	cvt.u32.u16 	%r1400, %rs4;
	shl.b32 	%r1401, %r1400, 16;
	or.b32  	%r911, %r1401, %r1399;
	cvt.u32.u16 	%r1402, %rs5;
	cvt.u32.u16 	%r1403, %rs6;
	shl.b32 	%r1404, %r1403, 16;
	or.b32  	%r912, %r1404, %r1402;
	cvt.u32.u16 	%r1405, %rs7;
	cvt.u32.u16 	%r1406, %rs8;
	shl.b32 	%r1407, %r1406, 16;
	or.b32  	%r913, %r1407, %r1405;
	cvt.u32.u16 	%r1408, %rs9;
	cvt.u32.u16 	%r1409, %rs10;
	shl.b32 	%r1410, %r1409, 16;
	or.b32  	%r1006, %r1410, %r1408;
	cvt.u32.u16 	%r1411, %rs11;
	cvt.u32.u16 	%r1412, %rs12;
	shl.b32 	%r1413, %r1412, 16;
	or.b32  	%r1007, %r1413, %r1411;
	cvt.u32.u16 	%r1414, %rs13;
	cvt.u32.u16 	%r1415, %rs14;
	shl.b32 	%r1416, %r1415, 16;
	or.b32  	%r1008, %r1416, %r1414;
	cvt.u32.u16 	%r1417, %rs15;
	cvt.u32.u16 	%r1418, %rs16;
	shl.b32 	%r1419, %r1418, 16;
	or.b32  	%r1009, %r1419, %r1417;
	cvt.u32.u16 	%r1420, %rs17;
	cvt.u32.u16 	%r1421, %rs18;
	shl.b32 	%r1422, %r1421, 16;
	or.b32  	%r1102, %r1422, %r1420;
	cvt.u32.u16 	%r1423, %rs19;
	cvt.u32.u16 	%r1424, %rs20;
	shl.b32 	%r1425, %r1424, 16;
	or.b32  	%r1103, %r1425, %r1423;
	cvt.u32.u16 	%r1426, %rs21;
	cvt.u32.u16 	%r1427, %rs22;
	shl.b32 	%r1428, %r1427, 16;
	or.b32  	%r1104, %r1428, %r1426;
	cvt.u32.u16 	%r1429, %rs23;
	cvt.u32.u16 	%r1430, %rs24;
	shl.b32 	%r1431, %r1430, 16;
	or.b32  	%r1105, %r1431, %r1429;
	cvt.u32.u16 	%r1432, %rs25;
	cvt.u32.u16 	%r1433, %rs26;
	shl.b32 	%r1434, %r1433, 16;
	or.b32  	%r1198, %r1434, %r1432;
	cvt.u32.u16 	%r1435, %rs27;
	cvt.u32.u16 	%r1436, %rs28;
	shl.b32 	%r1437, %r1436, 16;
	or.b32  	%r1199, %r1437, %r1435;
	cvt.u32.u16 	%r1438, %rs29;
	cvt.u32.u16 	%r1439, %rs30;
	shl.b32 	%r1440, %r1439, 16;
	or.b32  	%r1200, %r1440, %r1438;
	cvt.u32.u16 	%r1441, %rs31;
	cvt.u32.u16 	%r1442, %rs32;
	shl.b32 	%r1443, %r1442, 16;
	or.b32  	%r1201, %r1443, %r1441;
	cvt.u32.u16 	%r1444, %rs33;
	cvt.u32.u16 	%r1445, %rs34;
	shl.b32 	%r1446, %r1445, 16;
	or.b32  	%r958, %r1446, %r1444;
	cvt.u32.u16 	%r1447, %rs35;
	cvt.u32.u16 	%r1448, %rs36;
	shl.b32 	%r1449, %r1448, 16;
	or.b32  	%r959, %r1449, %r1447;
	cvt.u32.u16 	%r1450, %rs37;
	cvt.u32.u16 	%r1451, %rs38;
	shl.b32 	%r1452, %r1451, 16;
	or.b32  	%r960, %r1452, %r1450;
	cvt.u32.u16 	%r1453, %rs39;
	cvt.u32.u16 	%r1454, %rs40;
	shl.b32 	%r1455, %r1454, 16;
	or.b32  	%r961, %r1455, %r1453;
	cvt.u32.u16 	%r1456, %rs41;
	cvt.u32.u16 	%r1457, %rs42;
	shl.b32 	%r1458, %r1457, 16;
	or.b32  	%r1054, %r1458, %r1456;
	cvt.u32.u16 	%r1459, %rs43;
	cvt.u32.u16 	%r1460, %rs44;
	shl.b32 	%r1461, %r1460, 16;
	or.b32  	%r1055, %r1461, %r1459;
	cvt.u32.u16 	%r1462, %rs45;
	cvt.u32.u16 	%r1463, %rs46;
	shl.b32 	%r1464, %r1463, 16;
	or.b32  	%r1056, %r1464, %r1462;
	cvt.u32.u16 	%r1465, %rs47;
	cvt.u32.u16 	%r1466, %rs48;
	shl.b32 	%r1467, %r1466, 16;
	or.b32  	%r1057, %r1467, %r1465;
	cvt.u32.u16 	%r1468, %rs49;
	cvt.u32.u16 	%r1469, %rs50;
	shl.b32 	%r1470, %r1469, 16;
	or.b32  	%r1150, %r1470, %r1468;
	cvt.u32.u16 	%r1471, %rs51;
	cvt.u32.u16 	%r1472, %rs52;
	shl.b32 	%r1473, %r1472, 16;
	or.b32  	%r1151, %r1473, %r1471;
	cvt.u32.u16 	%r1474, %rs53;
	cvt.u32.u16 	%r1475, %rs54;
	shl.b32 	%r1476, %r1475, 16;
	or.b32  	%r1152, %r1476, %r1474;
	cvt.u32.u16 	%r1477, %rs55;
	cvt.u32.u16 	%r1478, %rs56;
	shl.b32 	%r1479, %r1478, 16;
	or.b32  	%r1153, %r1479, %r1477;
	cvt.u32.u16 	%r1480, %rs57;
	cvt.u32.u16 	%r1481, %rs58;
	shl.b32 	%r1482, %r1481, 16;
	or.b32  	%r1246, %r1482, %r1480;
	cvt.u32.u16 	%r1483, %rs59;
	cvt.u32.u16 	%r1484, %rs60;
	shl.b32 	%r1485, %r1484, 16;
	or.b32  	%r1247, %r1485, %r1483;
	cvt.u32.u16 	%r1486, %rs61;
	cvt.u32.u16 	%r1487, %rs62;
	shl.b32 	%r1488, %r1487, 16;
	or.b32  	%r1248, %r1488, %r1486;
	cvt.u32.u16 	%r1489, %rs63;
	cvt.u32.u16 	%r1490, %rs64;
	shl.b32 	%r1491, %r1490, 16;
	or.b32  	%r1249, %r1491, %r1489;
	.loc	1 437 20
	add.s32 	%r834, %r1882, %r1492;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r914, %r915, %r920, %r921 }, [ %r834 + 0 ];
	// end inline asm
	add.s32 	%r839, %r834, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1010, %r1011, %r1016, %r1017 }, [ %r839 + 0 ];
	// end inline asm
	add.s32 	%r844, %r834, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1106, %r1107, %r1112, %r1113 }, [ %r844 + 0 ];
	// end inline asm
	add.s32 	%r849, %r834, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1202, %r1203, %r1208, %r1209 }, [ %r849 + 0 ];
	// end inline asm
	add.s32 	%r854, %r1882, %r1493;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r926, %r927, %r932, %r933 }, [ %r854 + 0 ];
	// end inline asm
	add.s32 	%r859, %r854, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1022, %r1023, %r1028, %r1029 }, [ %r859 + 0 ];
	// end inline asm
	add.s32 	%r864, %r854, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1118, %r1119, %r1124, %r1125 }, [ %r864 + 0 ];
	// end inline asm
	add.s32 	%r869, %r854, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1214, %r1215, %r1220, %r1221 }, [ %r869 + 0 ];
	// end inline asm
	add.s32 	%r874, %r1882, %r1494;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r938, %r939, %r944, %r945 }, [ %r874 + 0 ];
	// end inline asm
	add.s32 	%r879, %r874, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1034, %r1035, %r1040, %r1041 }, [ %r879 + 0 ];
	// end inline asm
	add.s32 	%r884, %r874, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1130, %r1131, %r1136, %r1137 }, [ %r884 + 0 ];
	// end inline asm
	add.s32 	%r889, %r874, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1226, %r1227, %r1232, %r1233 }, [ %r889 + 0 ];
	// end inline asm
	add.s32 	%r894, %r1882, %r1495;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r950, %r951, %r956, %r957 }, [ %r894 + 0 ];
	// end inline asm
	add.s32 	%r899, %r894, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1046, %r1047, %r1052, %r1053 }, [ %r899 + 0 ];
	// end inline asm
	add.s32 	%r904, %r894, 4096;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1142, %r1143, %r1148, %r1149 }, [ %r904 + 0 ];
	// end inline asm
	add.s32 	%r909, %r894, 6144;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.trans.shared.b16 { %r1238, %r1239, %r1244, %r1245 }, [ %r909 + 0 ];
	// end inline asm
	.loc	1 440 44
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f870, %f871, %f872, %f873 }, { %r910, %r911, %r912, %r913 }, { %r914, %r915 }, { %f870, %f871, %f872, %f873 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f878, %f879, %f880, %f881 }, { %r910, %r911, %r912, %r913 }, { %r920, %r921 }, { %f878, %f879, %f880, %f881 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f886, %f887, %f888, %f889 }, { %r910, %r911, %r912, %r913 }, { %r926, %r927 }, { %f886, %f887, %f888, %f889 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f894, %f895, %f896, %f897 }, { %r910, %r911, %r912, %r913 }, { %r932, %r933 }, { %f894, %f895, %f896, %f897 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f902, %f903, %f904, %f905 }, { %r910, %r911, %r912, %r913 }, { %r938, %r939 }, { %f902, %f903, %f904, %f905 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f910, %f911, %f912, %f913 }, { %r910, %r911, %r912, %r913 }, { %r944, %r945 }, { %f910, %f911, %f912, %f913 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f918, %f919, %f920, %f921 }, { %r910, %r911, %r912, %r913 }, { %r950, %r951 }, { %f918, %f919, %f920, %f921 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f926, %f927, %f928, %f929 }, { %r910, %r911, %r912, %r913 }, { %r956, %r957 }, { %f926, %f927, %f928, %f929 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f934, %f935, %f936, %f937 }, { %r958, %r959, %r960, %r961 }, { %r914, %r915 }, { %f934, %f935, %f936, %f937 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f942, %f943, %f944, %f945 }, { %r958, %r959, %r960, %r961 }, { %r920, %r921 }, { %f942, %f943, %f944, %f945 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f950, %f951, %f952, %f953 }, { %r958, %r959, %r960, %r961 }, { %r926, %r927 }, { %f950, %f951, %f952, %f953 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f958, %f959, %f960, %f961 }, { %r958, %r959, %r960, %r961 }, { %r932, %r933 }, { %f958, %f959, %f960, %f961 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f966, %f967, %f968, %f969 }, { %r958, %r959, %r960, %r961 }, { %r938, %r939 }, { %f966, %f967, %f968, %f969 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f974, %f975, %f976, %f977 }, { %r958, %r959, %r960, %r961 }, { %r944, %r945 }, { %f974, %f975, %f976, %f977 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f982, %f983, %f984, %f985 }, { %r958, %r959, %r960, %r961 }, { %r950, %r951 }, { %f982, %f983, %f984, %f985 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f990, %f991, %f992, %f993 }, { %r958, %r959, %r960, %r961 }, { %r956, %r957 }, { %f990, %f991, %f992, %f993 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f870, %f871, %f872, %f873 }, { %r1006, %r1007, %r1008, %r1009 }, { %r1010, %r1011 }, { %f870, %f871, %f872, %f873 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f878, %f879, %f880, %f881 }, { %r1006, %r1007, %r1008, %r1009 }, { %r1016, %r1017 }, { %f878, %f879, %f880, %f881 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f886, %f887, %f888, %f889 }, { %r1006, %r1007, %r1008, %r1009 }, { %r1022, %r1023 }, { %f886, %f887, %f888, %f889 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f894, %f895, %f896, %f897 }, { %r1006, %r1007, %r1008, %r1009 }, { %r1028, %r1029 }, { %f894, %f895, %f896, %f897 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f902, %f903, %f904, %f905 }, { %r1006, %r1007, %r1008, %r1009 }, { %r1034, %r1035 }, { %f902, %f903, %f904, %f905 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f910, %f911, %f912, %f913 }, { %r1006, %r1007, %r1008, %r1009 }, { %r1040, %r1041 }, { %f910, %f911, %f912, %f913 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f918, %f919, %f920, %f921 }, { %r1006, %r1007, %r1008, %r1009 }, { %r1046, %r1047 }, { %f918, %f919, %f920, %f921 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f926, %f927, %f928, %f929 }, { %r1006, %r1007, %r1008, %r1009 }, { %r1052, %r1053 }, { %f926, %f927, %f928, %f929 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f934, %f935, %f936, %f937 }, { %r1054, %r1055, %r1056, %r1057 }, { %r1010, %r1011 }, { %f934, %f935, %f936, %f937 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f942, %f943, %f944, %f945 }, { %r1054, %r1055, %r1056, %r1057 }, { %r1016, %r1017 }, { %f942, %f943, %f944, %f945 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f950, %f951, %f952, %f953 }, { %r1054, %r1055, %r1056, %r1057 }, { %r1022, %r1023 }, { %f950, %f951, %f952, %f953 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f958, %f959, %f960, %f961 }, { %r1054, %r1055, %r1056, %r1057 }, { %r1028, %r1029 }, { %f958, %f959, %f960, %f961 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f966, %f967, %f968, %f969 }, { %r1054, %r1055, %r1056, %r1057 }, { %r1034, %r1035 }, { %f966, %f967, %f968, %f969 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f974, %f975, %f976, %f977 }, { %r1054, %r1055, %r1056, %r1057 }, { %r1040, %r1041 }, { %f974, %f975, %f976, %f977 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f982, %f983, %f984, %f985 }, { %r1054, %r1055, %r1056, %r1057 }, { %r1046, %r1047 }, { %f982, %f983, %f984, %f985 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f990, %f991, %f992, %f993 }, { %r1054, %r1055, %r1056, %r1057 }, { %r1052, %r1053 }, { %f990, %f991, %f992, %f993 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f870, %f871, %f872, %f873 }, { %r1102, %r1103, %r1104, %r1105 }, { %r1106, %r1107 }, { %f870, %f871, %f872, %f873 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f878, %f879, %f880, %f881 }, { %r1102, %r1103, %r1104, %r1105 }, { %r1112, %r1113 }, { %f878, %f879, %f880, %f881 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f886, %f887, %f888, %f889 }, { %r1102, %r1103, %r1104, %r1105 }, { %r1118, %r1119 }, { %f886, %f887, %f888, %f889 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f894, %f895, %f896, %f897 }, { %r1102, %r1103, %r1104, %r1105 }, { %r1124, %r1125 }, { %f894, %f895, %f896, %f897 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f902, %f903, %f904, %f905 }, { %r1102, %r1103, %r1104, %r1105 }, { %r1130, %r1131 }, { %f902, %f903, %f904, %f905 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f910, %f911, %f912, %f913 }, { %r1102, %r1103, %r1104, %r1105 }, { %r1136, %r1137 }, { %f910, %f911, %f912, %f913 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f918, %f919, %f920, %f921 }, { %r1102, %r1103, %r1104, %r1105 }, { %r1142, %r1143 }, { %f918, %f919, %f920, %f921 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f926, %f927, %f928, %f929 }, { %r1102, %r1103, %r1104, %r1105 }, { %r1148, %r1149 }, { %f926, %f927, %f928, %f929 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f934, %f935, %f936, %f937 }, { %r1150, %r1151, %r1152, %r1153 }, { %r1106, %r1107 }, { %f934, %f935, %f936, %f937 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f942, %f943, %f944, %f945 }, { %r1150, %r1151, %r1152, %r1153 }, { %r1112, %r1113 }, { %f942, %f943, %f944, %f945 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f950, %f951, %f952, %f953 }, { %r1150, %r1151, %r1152, %r1153 }, { %r1118, %r1119 }, { %f950, %f951, %f952, %f953 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f958, %f959, %f960, %f961 }, { %r1150, %r1151, %r1152, %r1153 }, { %r1124, %r1125 }, { %f958, %f959, %f960, %f961 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f966, %f967, %f968, %f969 }, { %r1150, %r1151, %r1152, %r1153 }, { %r1130, %r1131 }, { %f966, %f967, %f968, %f969 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f974, %f975, %f976, %f977 }, { %r1150, %r1151, %r1152, %r1153 }, { %r1136, %r1137 }, { %f974, %f975, %f976, %f977 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f982, %f983, %f984, %f985 }, { %r1150, %r1151, %r1152, %r1153 }, { %r1142, %r1143 }, { %f982, %f983, %f984, %f985 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f990, %f991, %f992, %f993 }, { %r1150, %r1151, %r1152, %r1153 }, { %r1148, %r1149 }, { %f990, %f991, %f992, %f993 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f870, %f871, %f872, %f873 }, { %r1198, %r1199, %r1200, %r1201 }, { %r1202, %r1203 }, { %f870, %f871, %f872, %f873 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f878, %f879, %f880, %f881 }, { %r1198, %r1199, %r1200, %r1201 }, { %r1208, %r1209 }, { %f878, %f879, %f880, %f881 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f886, %f887, %f888, %f889 }, { %r1198, %r1199, %r1200, %r1201 }, { %r1214, %r1215 }, { %f886, %f887, %f888, %f889 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f894, %f895, %f896, %f897 }, { %r1198, %r1199, %r1200, %r1201 }, { %r1220, %r1221 }, { %f894, %f895, %f896, %f897 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f902, %f903, %f904, %f905 }, { %r1198, %r1199, %r1200, %r1201 }, { %r1226, %r1227 }, { %f902, %f903, %f904, %f905 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f910, %f911, %f912, %f913 }, { %r1198, %r1199, %r1200, %r1201 }, { %r1232, %r1233 }, { %f910, %f911, %f912, %f913 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f918, %f919, %f920, %f921 }, { %r1198, %r1199, %r1200, %r1201 }, { %r1238, %r1239 }, { %f918, %f919, %f920, %f921 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f926, %f927, %f928, %f929 }, { %r1198, %r1199, %r1200, %r1201 }, { %r1244, %r1245 }, { %f926, %f927, %f928, %f929 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f934, %f935, %f936, %f937 }, { %r1246, %r1247, %r1248, %r1249 }, { %r1202, %r1203 }, { %f934, %f935, %f936, %f937 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f942, %f943, %f944, %f945 }, { %r1246, %r1247, %r1248, %r1249 }, { %r1208, %r1209 }, { %f942, %f943, %f944, %f945 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f950, %f951, %f952, %f953 }, { %r1246, %r1247, %r1248, %r1249 }, { %r1214, %r1215 }, { %f950, %f951, %f952, %f953 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f958, %f959, %f960, %f961 }, { %r1246, %r1247, %r1248, %r1249 }, { %r1220, %r1221 }, { %f958, %f959, %f960, %f961 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f966, %f967, %f968, %f969 }, { %r1246, %r1247, %r1248, %r1249 }, { %r1226, %r1227 }, { %f966, %f967, %f968, %f969 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f974, %f975, %f976, %f977 }, { %r1246, %r1247, %r1248, %r1249 }, { %r1232, %r1233 }, { %f974, %f975, %f976, %f977 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f982, %f983, %f984, %f985 }, { %r1246, %r1247, %r1248, %r1249 }, { %r1238, %r1239 }, { %f982, %f983, %f984, %f985 };
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 { %f990, %f991, %f992, %f993 }, { %r1246, %r1247, %r1248, %r1249 }, { %r1244, %r1245 }, { %f990, %f991, %f992, %f993 };
	// end inline asm
	.loc	1 281 40
	add.s32 	%r1886, %r1886, 1;
	.loc	1 325 33
	shr.u32 	%r1496, %r1886, 24;
	.loc	1 326 38
	mul.wide.u32 	%rd121, %r1496, 4;
	add.s64 	%rd111, %rd23, %rd121;
	.loc	1 326 24
	// begin inline asm
	mov.u32 %r1294, 0x0;
	@%p30 ld.global.L1::evict_last.b32 { %r1294 }, [ %rd111 + 0 ];
	// end inline asm
	.loc	1 327 109
	add.s32 	%r1497, %r1496, 1;
	.loc	1 327 113
	setp.lt.s32 	%p45, %r1497, %r92;
	.loc	1 327 55
	add.s64 	%rd112, %rd111, 4;
	.loc	1 281 40
	and.pred  	%p31, %p30, %p45;
	.loc	1 327 25
	// begin inline asm
	mov.u32 %r1295, 0x0;
	@%p31 ld.global.L1::evict_last.b32 { %r1295 }, [ %rd112 + 0 ];
	// end inline asm
	.loc	1 281 40
	add.s32 	%r1498, %r1885, 1;
	setp.lt.s32 	%p46, %r1498, 2;
	selp.b32 	%r1885, %r1498, 0, %p46;
	.loc	1 364 20
	add.s64 	%rd122, %rd140, %rd5;
	add.s64 	%rd113, %rd122, 16384;
	add.s64 	%rd114, %rd122, 18432;
	add.s64 	%rd115, %rd122, 20480;
	add.s64 	%rd116, %rd122, 22528;
	shl.b32 	%r1499, %r1885, 13;
	add.s32 	%r1502, %r205, %r1499;
	bar.sync 	0;
	add.s32 	%r1296, %r1502, %r169;
	add.s32 	%r1298, %r1502, %r173;
	add.s32 	%r1300, %r1502, %r176;
	add.s32 	%r1302, %r1502, %r179;
	selp.b32 	%r1297, 16, 0, %p40;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r1296 + 0 ], [ %rd113 + 0 ], 0x10, %r1297;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r1298 + 0 ], [ %rd114 + 0 ], 0x10, %r1297;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r1300 + 0 ], [ %rd115 + 0 ], 0x10, %r1297;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r1302 + 0 ], [ %rd116 + 0 ], 0x10, %r1297;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 437 20
	add.s64 	%rd123, %rd141, %rd5;
	add.s64 	%rd117, %rd123, 16384;
	add.s64 	%rd118, %rd123, 18432;
	add.s64 	%rd119, %rd123, 20480;
	add.s64 	%rd120, %rd123, 22528;
	add.s32 	%r1508, %r206, %r1499;
	add.s32 	%r1304, %r1508, %r169;
	add.s32 	%r1306, %r1508, %r173;
	add.s32 	%r1308, %r1508, %r176;
	add.s32 	%r1310, %r1508, %r179;
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r1304 + 0 ], [ %rd117 + 0 ], 0x10, %r1297;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r1306 + 0 ], [ %rd118 + 0 ], 0x10, %r1297;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r1308 + 0 ], [ %rd119 + 0 ], 0x10, %r1297;
	// end inline asm
	// begin inline asm
	@%p1 cp.async.cg.shared.global [ %r1310 + 0 ], [ %rd120 + 0 ], 0x10, %r1297;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 281 40
	add.s32 	%r1509, %r1884, 1;
	setp.lt.s32 	%p47, %r1509, 2;
	selp.b32 	%r1884, %r1509, 0, %p47;
	.loc	1 364 20
	shl.b32 	%r1510, %r1884, 13;
	add.s32 	%r1883, %r205, %r1510;
	// begin inline asm
	cp.async.wait_group 0x2;
	// end inline asm
	bar.sync 	0;
	.loc	1 437 20
	add.s32 	%r1882, %r206, %r1510;
	.loc	1 281 40
	add.s32 	%r1881, %r1881, 64;
	add.s32 	%r1880, %r1880, 64;
	add.s64 	%rd141, %rd141, 8192;
	add.s64 	%rd140, %rd140, 8192;
	setp.lt.s32 	%p48, %r1886, %r21;
	mov.f32 	%f1847, %f74;
	mov.f32 	%f1848, %f75;
	mov.f32 	%f1849, %f76;
	mov.f32 	%f1850, %f77;
	@%p48 bra 	$L__BB0_2;
$L__tmp6:
$L__BB0_3:
	.loc	1 0 40
	cvt.u32.u64 	%r1799, %rd1;
	cvt.u32.u64 	%r1800, %rd2;
	.loc	1 115 33
	or.b32  	%r1801, %r1, %r12;
	or.b32  	%r1802, %r1, %r11;
	or.b32  	%r1803, %r1, %r10;
	or.b32  	%r1804, %r1, %r9;
	or.b32  	%r1805, %r1, %r8;
	or.b32  	%r1806, %r1, %r7;
	or.b32  	%r1807, %r1, %r6;
	or.b32  	%r1808, %r1, %r1800;
$L__tmp7:
	.loc	1 281 40
	// begin inline asm
	cp.async.wait_group 0x0;
	// end inline asm
	bar.sync 	0;
$L__tmp8:
	.loc	1 218 26
	setp.eq.f32 	%p57, %f1915, 0f00000000;
	setp.eq.f32 	%p58, %f1916, 0f00000000;
	setp.eq.f32 	%p59, %f1917, 0f00000000;
	setp.eq.f32 	%p60, %f1918, 0f00000000;
	.loc	1 218 34
	selp.f32 	%f1775, 0f3F800000, %f1915, %p57;
	selp.f32 	%f1776, 0f3F800000, %f1916, %p58;
	selp.f32 	%f1777, 0f3F800000, %f1917, %p59;
	selp.f32 	%f1778, 0f3F800000, %f1918, %p60;
	.loc	1 220 16
	mov.b32 	%r1513, %f1775;
	mov.b32 	%r1512, %f870;
	// begin inline asm
	div.full.f32 %r1703, %r1512, %r1513;
	// end inline asm
	mov.b32 	%r1515, %f871;
	// begin inline asm
	div.full.f32 %r1704, %r1515, %r1513;
	// end inline asm
	mov.b32 	%r1519, %f1776;
	mov.b32 	%r1518, %f872;
	// begin inline asm
	div.full.f32 %r1705, %r1518, %r1519;
	// end inline asm
	mov.b32 	%r1521, %f873;
	// begin inline asm
	div.full.f32 %r1706, %r1521, %r1519;
	// end inline asm
	mov.b32 	%r1524, %f878;
	// begin inline asm
	div.full.f32 %r1707, %r1524, %r1513;
	// end inline asm
	mov.b32 	%r1527, %f879;
	// begin inline asm
	div.full.f32 %r1708, %r1527, %r1513;
	// end inline asm
	mov.b32 	%r1530, %f880;
	// begin inline asm
	div.full.f32 %r1709, %r1530, %r1519;
	// end inline asm
	mov.b32 	%r1533, %f881;
	// begin inline asm
	div.full.f32 %r1710, %r1533, %r1519;
	// end inline asm
	mov.b32 	%r1536, %f886;
	// begin inline asm
	div.full.f32 %r1711, %r1536, %r1513;
	// end inline asm
	mov.b32 	%r1539, %f887;
	// begin inline asm
	div.full.f32 %r1712, %r1539, %r1513;
	// end inline asm
	mov.b32 	%r1542, %f888;
	// begin inline asm
	div.full.f32 %r1713, %r1542, %r1519;
	// end inline asm
	mov.b32 	%r1545, %f889;
	// begin inline asm
	div.full.f32 %r1714, %r1545, %r1519;
	// end inline asm
	mov.b32 	%r1548, %f894;
	// begin inline asm
	div.full.f32 %r1715, %r1548, %r1513;
	// end inline asm
	mov.b32 	%r1551, %f895;
	// begin inline asm
	div.full.f32 %r1716, %r1551, %r1513;
	// end inline asm
	mov.b32 	%r1554, %f896;
	// begin inline asm
	div.full.f32 %r1717, %r1554, %r1519;
	// end inline asm
	mov.b32 	%r1557, %f897;
	// begin inline asm
	div.full.f32 %r1718, %r1557, %r1519;
	// end inline asm
	mov.b32 	%r1560, %f902;
	// begin inline asm
	div.full.f32 %r1719, %r1560, %r1513;
	// end inline asm
	mov.b32 	%r1563, %f903;
	// begin inline asm
	div.full.f32 %r1720, %r1563, %r1513;
	// end inline asm
	mov.b32 	%r1566, %f904;
	// begin inline asm
	div.full.f32 %r1721, %r1566, %r1519;
	// end inline asm
	mov.b32 	%r1569, %f905;
	// begin inline asm
	div.full.f32 %r1722, %r1569, %r1519;
	// end inline asm
	mov.b32 	%r1572, %f910;
	// begin inline asm
	div.full.f32 %r1723, %r1572, %r1513;
	// end inline asm
	mov.b32 	%r1575, %f911;
	// begin inline asm
	div.full.f32 %r1724, %r1575, %r1513;
	// end inline asm
	mov.b32 	%r1578, %f912;
	// begin inline asm
	div.full.f32 %r1725, %r1578, %r1519;
	// end inline asm
	mov.b32 	%r1581, %f913;
	// begin inline asm
	div.full.f32 %r1726, %r1581, %r1519;
	// end inline asm
	mov.b32 	%r1584, %f918;
	// begin inline asm
	div.full.f32 %r1727, %r1584, %r1513;
	// end inline asm
	mov.b32 	%r1587, %f919;
	// begin inline asm
	div.full.f32 %r1728, %r1587, %r1513;
	// end inline asm
	mov.b32 	%r1590, %f920;
	// begin inline asm
	div.full.f32 %r1729, %r1590, %r1519;
	// end inline asm
	mov.b32 	%r1593, %f921;
	// begin inline asm
	div.full.f32 %r1730, %r1593, %r1519;
	// end inline asm
	mov.b32 	%r1596, %f926;
	// begin inline asm
	div.full.f32 %r1731, %r1596, %r1513;
	// end inline asm
	mov.b32 	%r1599, %f927;
	// begin inline asm
	div.full.f32 %r1732, %r1599, %r1513;
	// end inline asm
	mov.b32 	%r1602, %f928;
	// begin inline asm
	div.full.f32 %r1733, %r1602, %r1519;
	// end inline asm
	mov.b32 	%r1605, %f929;
	// begin inline asm
	div.full.f32 %r1734, %r1605, %r1519;
	// end inline asm
	mov.b32 	%r1609, %f1777;
	mov.b32 	%r1608, %f934;
	// begin inline asm
	div.full.f32 %r1735, %r1608, %r1609;
	// end inline asm
	mov.b32 	%r1611, %f935;
	// begin inline asm
	div.full.f32 %r1736, %r1611, %r1609;
	// end inline asm
	mov.b32 	%r1615, %f1778;
	mov.b32 	%r1614, %f936;
	// begin inline asm
	div.full.f32 %r1737, %r1614, %r1615;
	// end inline asm
	mov.b32 	%r1617, %f937;
	// begin inline asm
	div.full.f32 %r1738, %r1617, %r1615;
	// end inline asm
	mov.b32 	%r1620, %f942;
	// begin inline asm
	div.full.f32 %r1739, %r1620, %r1609;
	// end inline asm
	mov.b32 	%r1623, %f943;
	// begin inline asm
	div.full.f32 %r1740, %r1623, %r1609;
	// end inline asm
	mov.b32 	%r1626, %f944;
	// begin inline asm
	div.full.f32 %r1741, %r1626, %r1615;
	// end inline asm
	mov.b32 	%r1629, %f945;
	// begin inline asm
	div.full.f32 %r1742, %r1629, %r1615;
	// end inline asm
	mov.b32 	%r1632, %f950;
	// begin inline asm
	div.full.f32 %r1743, %r1632, %r1609;
	// end inline asm
	mov.b32 	%r1635, %f951;
	// begin inline asm
	div.full.f32 %r1744, %r1635, %r1609;
	// end inline asm
	mov.b32 	%r1638, %f952;
	// begin inline asm
	div.full.f32 %r1745, %r1638, %r1615;
	// end inline asm
	mov.b32 	%r1641, %f953;
	// begin inline asm
	div.full.f32 %r1746, %r1641, %r1615;
	// end inline asm
	mov.b32 	%r1644, %f958;
	// begin inline asm
	div.full.f32 %r1747, %r1644, %r1609;
	// end inline asm
	mov.b32 	%r1647, %f959;
	// begin inline asm
	div.full.f32 %r1748, %r1647, %r1609;
	// end inline asm
	mov.b32 	%r1650, %f960;
	// begin inline asm
	div.full.f32 %r1749, %r1650, %r1615;
	// end inline asm
	mov.b32 	%r1653, %f961;
	// begin inline asm
	div.full.f32 %r1750, %r1653, %r1615;
	// end inline asm
	mov.b32 	%r1656, %f966;
	// begin inline asm
	div.full.f32 %r1751, %r1656, %r1609;
	// end inline asm
	mov.b32 	%r1659, %f967;
	// begin inline asm
	div.full.f32 %r1752, %r1659, %r1609;
	// end inline asm
	mov.b32 	%r1662, %f968;
	// begin inline asm
	div.full.f32 %r1753, %r1662, %r1615;
	// end inline asm
	mov.b32 	%r1665, %f969;
	// begin inline asm
	div.full.f32 %r1754, %r1665, %r1615;
	// end inline asm
	mov.b32 	%r1668, %f974;
	// begin inline asm
	div.full.f32 %r1755, %r1668, %r1609;
	// end inline asm
	mov.b32 	%r1671, %f975;
	// begin inline asm
	div.full.f32 %r1756, %r1671, %r1609;
	// end inline asm
	mov.b32 	%r1674, %f976;
	// begin inline asm
	div.full.f32 %r1757, %r1674, %r1615;
	// end inline asm
	mov.b32 	%r1677, %f977;
	// begin inline asm
	div.full.f32 %r1758, %r1677, %r1615;
	// end inline asm
	mov.b32 	%r1680, %f982;
	// begin inline asm
	div.full.f32 %r1759, %r1680, %r1609;
	// end inline asm
	mov.b32 	%r1683, %f983;
	// begin inline asm
	div.full.f32 %r1760, %r1683, %r1609;
	// end inline asm
	mov.b32 	%r1686, %f984;
	// begin inline asm
	div.full.f32 %r1761, %r1686, %r1615;
	// end inline asm
	mov.b32 	%r1689, %f985;
	// begin inline asm
	div.full.f32 %r1762, %r1689, %r1615;
	// end inline asm
	mov.b32 	%r1692, %f990;
	// begin inline asm
	div.full.f32 %r1763, %r1692, %r1609;
	// end inline asm
	mov.b32 	%r1695, %f991;
	// begin inline asm
	div.full.f32 %r1764, %r1695, %r1609;
	// end inline asm
	mov.b32 	%r1698, %f992;
	// begin inline asm
	div.full.f32 %r1765, %r1698, %r1615;
	// end inline asm
	mov.b32 	%r1701, %f993;
	// begin inline asm
	div.full.f32 %r1766, %r1701, %r1615;
	// end inline asm
	.loc	1 226 19
	setp.lt.s32 	%p49, %r1808, 512;
	setp.lt.s32 	%p50, %r1807, 512;
	setp.lt.s32 	%p51, %r1806, 512;
	setp.lt.s32 	%p52, %r1805, 512;
	setp.lt.s32 	%p53, %r1804, 512;
	setp.lt.s32 	%p54, %r1803, 512;
	setp.lt.s32 	%p55, %r1802, 512;
	setp.lt.s32 	%p56, %r1801, 512;
	.loc	1 228 25
	shl.b32 	%r1809, %r1808, 6;
	shl.b32 	%r1810, %r1807, 6;
	shl.b32 	%r1811, %r1806, 6;
	shl.b32 	%r1812, %r1805, 6;
	shl.b32 	%r1813, %r1804, 6;
	shl.b32 	%r1814, %r1803, 6;
	shl.b32 	%r1815, %r1802, 6;
	shl.b32 	%r1816, %r1801, 6;
	.loc	1 228 22
	or.b32  	%r1817, %r1809, %r15;
	or.b32  	%r1818, %r1810, %r15;
	or.b32  	%r1819, %r1811, %r15;
	or.b32  	%r1820, %r1812, %r15;
	or.b32  	%r1821, %r1813, %r15;
	or.b32  	%r1822, %r1814, %r15;
	or.b32  	%r1823, %r1815, %r15;
	or.b32  	%r1824, %r1816, %r15;
	.loc	1 228 52
	add.s32 	%r1825, %r1799, %r1817;
	add.s32 	%r1826, %r1799, %r1818;
	add.s32 	%r1827, %r1799, %r1819;
	add.s32 	%r1828, %r1799, %r1820;
	add.s32 	%r1829, %r1799, %r1821;
	add.s32 	%r1830, %r1799, %r1822;
	add.s32 	%r1831, %r1799, %r1823;
	add.s32 	%r1832, %r1799, %r1824;
	.loc	1 229 25
	mul.wide.s32 	%rd132, %r1825, 2;
	add.s64 	%rd124, %rd14, %rd132;
	mul.wide.s32 	%rd133, %r1826, 2;
	add.s64 	%rd125, %rd14, %rd133;
	mul.wide.s32 	%rd134, %r1827, 2;
	add.s64 	%rd126, %rd14, %rd134;
	mul.wide.s32 	%rd135, %r1828, 2;
	add.s64 	%rd127, %rd14, %rd135;
	mul.wide.s32 	%rd136, %r1829, 2;
	add.s64 	%rd128, %rd14, %rd136;
	mul.wide.s32 	%rd137, %r1830, 2;
	add.s64 	%rd129, %rd14, %rd137;
	mul.wide.s32 	%rd138, %r1831, 2;
	add.s64 	%rd130, %rd14, %rd138;
	mul.wide.s32 	%rd139, %r1832, 2;
	add.s64 	%rd131, %rd14, %rd139;
	.loc	1 229 67
	// begin inline asm
	cvt.rn.bf16.f32 %rs65, %r1703;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs66, %r1704;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs67, %r1705;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs68, %r1706;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs69, %r1707;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs70, %r1708;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs71, %r1709;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs72, %r1710;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs73, %r1711;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs74, %r1712;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs75, %r1713;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs76, %r1714;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs77, %r1715;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs78, %r1716;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs79, %r1717;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs80, %r1718;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs81, %r1719;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs82, %r1720;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs83, %r1721;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs84, %r1722;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs85, %r1723;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs86, %r1724;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs87, %r1725;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs88, %r1726;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs89, %r1727;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs90, %r1728;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs91, %r1729;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs92, %r1730;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs93, %r1731;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs94, %r1732;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs95, %r1733;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs96, %r1734;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs97, %r1735;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs98, %r1736;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs99, %r1737;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs100, %r1738;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs101, %r1739;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs102, %r1740;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs103, %r1741;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs104, %r1742;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs105, %r1743;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs106, %r1744;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs107, %r1745;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs108, %r1746;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs109, %r1747;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs110, %r1748;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs111, %r1749;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs112, %r1750;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs113, %r1751;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs114, %r1752;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs115, %r1753;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs116, %r1754;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs117, %r1755;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs118, %r1756;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs119, %r1757;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs120, %r1758;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs121, %r1759;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs122, %r1760;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs123, %r1761;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs124, %r1762;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs125, %r1763;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs126, %r1764;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs127, %r1765;
	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs128, %r1766;
	// end inline asm
	and.b32  	%r1833, %r4, 3;
	shr.u32 	%r1834, %r3, 2;
	shl.b32 	%r1835, %r1833, 4;
	or.b32  	%r1836, %r1835, %r1834;
	mul.lo.s32 	%r1837, %r1836, 72;
	or.b32  	%r1838, %r1837, %r22;
	shl.b32 	%r1839, %r1838, 1;
	add.s32 	%r1841, %r170, %r1839;
	mov.b32 	%r1842, {%rs65, %rs66};
	st.shared.u32 	[%r1841], %r1842;
	mov.b32 	%r1843, {%rs67, %rs68};
	st.shared.u32 	[%r1841+1152], %r1843;
	mov.b32 	%r1844, {%rs69, %rs70};
	st.shared.u32 	[%r1841+16], %r1844;
	mov.b32 	%r1845, {%rs71, %rs72};
	st.shared.u32 	[%r1841+1168], %r1845;
	mov.b32 	%r1846, {%rs73, %rs74};
	st.shared.u32 	[%r1841+32], %r1846;
	mov.b32 	%r1847, {%rs75, %rs76};
	st.shared.u32 	[%r1841+1184], %r1847;
	mov.b32 	%r1848, {%rs77, %rs78};
	st.shared.u32 	[%r1841+48], %r1848;
	mov.b32 	%r1849, {%rs79, %rs80};
	st.shared.u32 	[%r1841+1200], %r1849;
	mov.b32 	%r1850, {%rs81, %rs82};
	st.shared.u32 	[%r1841+64], %r1850;
	mov.b32 	%r1851, {%rs83, %rs84};
	st.shared.u32 	[%r1841+1216], %r1851;
	mov.b32 	%r1852, {%rs85, %rs86};
	st.shared.u32 	[%r1841+80], %r1852;
	mov.b32 	%r1853, {%rs87, %rs88};
	st.shared.u32 	[%r1841+1232], %r1853;
	mov.b32 	%r1854, {%rs89, %rs90};
	st.shared.u32 	[%r1841+96], %r1854;
	mov.b32 	%r1855, {%rs91, %rs92};
	st.shared.u32 	[%r1841+1248], %r1855;
	mov.b32 	%r1856, {%rs93, %rs94};
	st.shared.u32 	[%r1841+112], %r1856;
	mov.b32 	%r1857, {%rs95, %rs96};
	st.shared.u32 	[%r1841+1264], %r1857;
	bar.sync 	0;
	shr.u32 	%r1858, %r3, 3;
	shl.b32 	%r1859, %r1833, 2;
	or.b32  	%r1860, %r1859, %r1858;
	mad.lo.s32 	%r1861, %r1860, 72, %r15;
	shl.b32 	%r1862, %r1861, 1;
	add.s32 	%r1863, %r170, %r1862;
	ld.shared.v4.u32 	{%r1767, %r1768, %r1769, %r1770}, [%r1863];
	ld.shared.v4.u32 	{%r1771, %r1772, %r1773, %r1774}, [%r1863+2304];
	ld.shared.v4.u32 	{%r1775, %r1776, %r1777, %r1778}, [%r1863+4608];
	ld.shared.v4.u32 	{%r1779, %r1780, %r1781, %r1782}, [%r1863+6912];
	bar.sync 	0;
	mov.b32 	%r1864, {%rs97, %rs98};
	st.shared.u32 	[%r1841], %r1864;
	mov.b32 	%r1865, {%rs99, %rs100};
	st.shared.u32 	[%r1841+1152], %r1865;
	mov.b32 	%r1866, {%rs101, %rs102};
	st.shared.u32 	[%r1841+16], %r1866;
	mov.b32 	%r1867, {%rs103, %rs104};
	st.shared.u32 	[%r1841+1168], %r1867;
	mov.b32 	%r1868, {%rs105, %rs106};
	st.shared.u32 	[%r1841+32], %r1868;
	mov.b32 	%r1869, {%rs107, %rs108};
	st.shared.u32 	[%r1841+1184], %r1869;
	mov.b32 	%r1870, {%rs109, %rs110};
	st.shared.u32 	[%r1841+48], %r1870;
	mov.b32 	%r1871, {%rs111, %rs112};
	st.shared.u32 	[%r1841+1200], %r1871;
	mov.b32 	%r1872, {%rs113, %rs114};
	st.shared.u32 	[%r1841+64], %r1872;
	mov.b32 	%r1873, {%rs115, %rs116};
	st.shared.u32 	[%r1841+1216], %r1873;
	mov.b32 	%r1874, {%rs117, %rs118};
	st.shared.u32 	[%r1841+80], %r1874;
	mov.b32 	%r1875, {%rs119, %rs120};
	st.shared.u32 	[%r1841+1232], %r1875;
	mov.b32 	%r1876, {%rs121, %rs122};
	st.shared.u32 	[%r1841+96], %r1876;
	mov.b32 	%r1877, {%rs123, %rs124};
	st.shared.u32 	[%r1841+1248], %r1877;
	mov.b32 	%r1878, {%rs125, %rs126};
	st.shared.u32 	[%r1841+112], %r1878;
	mov.b32 	%r1879, {%rs127, %rs128};
	st.shared.u32 	[%r1841+1264], %r1879;
	bar.sync 	0;
	ld.shared.v4.u32 	{%r1783, %r1784, %r1785, %r1786}, [%r1863];
	ld.shared.v4.u32 	{%r1787, %r1788, %r1789, %r1790}, [%r1863+2304];
	ld.shared.v4.u32 	{%r1791, %r1792, %r1793, %r1794}, [%r1863+4608];
	ld.shared.v4.u32 	{%r1795, %r1796, %r1797, %r1798}, [%r1863+6912];
	// begin inline asm
	@%p49 st.global.v4.b32 [ %rd124 + 0 ], { %r1767, %r1768, %r1769, %r1770 };
	// end inline asm
	// begin inline asm
	@%p50 st.global.v4.b32 [ %rd125 + 0 ], { %r1771, %r1772, %r1773, %r1774 };
	// end inline asm
	// begin inline asm
	@%p51 st.global.v4.b32 [ %rd126 + 0 ], { %r1775, %r1776, %r1777, %r1778 };
	// end inline asm
	// begin inline asm
	@%p52 st.global.v4.b32 [ %rd127 + 0 ], { %r1779, %r1780, %r1781, %r1782 };
	// end inline asm
	// begin inline asm
	@%p53 st.global.v4.b32 [ %rd128 + 0 ], { %r1783, %r1784, %r1785, %r1786 };
	// end inline asm
	// begin inline asm
	@%p54 st.global.v4.b32 [ %rd129 + 0 ], { %r1787, %r1788, %r1789, %r1790 };
	// end inline asm
	// begin inline asm
	@%p55 st.global.v4.b32 [ %rd130 + 0 ], { %r1791, %r1792, %r1793, %r1794 };
	// end inline asm
	// begin inline asm
	@%p56 st.global.v4.b32 [ %rd131 + 0 ], { %r1795, %r1796, %r1797, %r1798 };
	// end inline asm
	.loc	1 232 4
	ret;
$L__tmp9:
$L__func_end0:

}
	.file	1 "/work/06112/byou/ls6/torchinductor_cache/or/corsi3wsbpqlv3cwkbbjykqfxr76blfd2m32cxzo3xr6hve5wa6i.py"
	.file	2 "/work/06112/byou/shared/miniforge3/envs/torch-2.5/lib/python3.12/site-packages/triton/language/standard.py"
	.section	.debug_abbrev
	{
.b8 1
.b8 17
.b8 1
.b8 37
.b8 8
.b8 19
.b8 5
.b8 3
.b8 8
.b8 16
.b8 6
.b8 27
.b8 8
.b8 17
.b8 1
.b8 18
.b8 1
.b8 0
.b8 0
.b8 2
.b8 46
.b8 0
.b8 3
.b8 8
.b8 32
.b8 11
.b8 0
.b8 0
.b8 3
.b8 46
.b8 1
.b8 17
.b8 1
.b8 18
.b8 1
.b8 49
.b8 19
.b8 0
.b8 0
.b8 4
.b8 29
.b8 0
.b8 49
.b8 19
.b8 17
.b8 1
.b8 18
.b8 1
.b8 88
.b8 11
.b8 89
.b8 11
.b8 87
.b8 11
.b8 0
.b8 0
.b8 0
	}
	.section	.debug_info
	{
.b32 194
.b8 2
.b8 0
.b32 .debug_abbrev
.b8 8
.b8 1
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2
.b8 0
.b8 99
.b8 111
.b8 114
.b8 115
.b8 105
.b8 51
.b8 119
.b8 115
.b8 98
.b8 112
.b8 113
.b8 108
.b8 118
.b8 51
.b8 99
.b8 119
.b8 107
.b8 98
.b8 98
.b8 106
.b8 121
.b8 107
.b8 113
.b8 102
.b8 120
.b8 114
.b8 55
.b8 54
.b8 98
.b8 108
.b8 102
.b8 100
.b8 50
.b8 109
.b8 51
.b8 50
.b8 99
.b8 120
.b8 122
.b8 111
.b8 51
.b8 120
.b8 114
.b8 54
.b8 104
.b8 118
.b8 101
.b8 53
.b8 119
.b8 97
.b8 54
.b8 105
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line
.b8 47
.b8 119
.b8 111
.b8 114
.b8 107
.b8 47
.b8 48
.b8 54
.b8 49
.b8 49
.b8 50
.b8 47
.b8 98
.b8 121
.b8 111
.b8 117
.b8 47
.b8 108
.b8 115
.b8 54
.b8 47
.b8 116
.b8 111
.b8 114
.b8 99
.b8 104
.b8 105
.b8 110
.b8 100
.b8 117
.b8 99
.b8 116
.b8 111
.b8 114
.b8 95
.b8 99
.b8 97
.b8 99
.b8 104
.b8 101
.b8 47
.b8 111
.b8 114
.b8 0
.b64 $L__func_begin0
.b64 $L__func_end0
.b8 2
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 95
.b8 0
.b8 1
.b8 3
.b64 $L__func_begin0
.b64 $L__func_end0
.b32 141
.b8 4
.b32 141
.b64 $L__tmp1
.b64 $L__tmp8
.b8 1
.b8 171
.b8 8
.b8 0
.b8 0
	}
	.section	.debug_loc	{	}
