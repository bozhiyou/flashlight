"""
batch_size=32, nheads=32, seqlen=512, headdim=64
                                      V
                                     ...
                                    |   |
                                    +---+\
                                    |   | \
                +--------+----      |   |  RBLOCK
             64 |        |  ... K   |   | /
          64    +--------+----      +---+/
       /+-----+  \RBLOCK/             64
XBLOCK0 |     |
       \+ - - +
        |     |
        + - - +
        |     |
          ...
           Q
"""
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties
from monkeypatch.experimental import block_reduction

@triton_heuristics.blockreduction(
    size_hints=[524288, 512],
    reduction_hint=ReductionHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*bf16', 2: '*bf16', 3: '*bf16', 4: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_blo_fused__softmax_add_bmm_mul_sub_0', 'mutated_arg_names': ['in_out_ptr0'], 'no_x_dim': False, 'num_load': 3, 'num_reduction': 4, 'backend_hash': 'A9C866B4A14FD3277824029365D703C2427B2E685E54EC9B3EF4ADC8D1EEAC1D', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'block_hints': {'xnumel': 512, 'xnumbl0': 512, 'RBLOCK': None, 'RBLOCK1': None}}
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, xnumel, XBLOCK0: tl.constexpr, xnumbl0: tl.constexpr, XBLOCK: tl.constexpr, RBLOCK: tl.constexpr, RBLOCK1: tl.constexpr):
    xnumel = 524288
    pid = tl.program_id(0)
    x5 = (pid // (32*xnumbl0)) % 32             # (unused) batch idx
    x1 = (pid // xnumbl0) % 1024                # flattened batch-head idx, 1024 = batch_size(32) * nheads(32)
    x4 = (pid // xnumbl0) % 32                  # head idx, `off_hq`/`off_h` in flex
    pid0 = pid % xnumbl0                        # flattened per-batch per-head idx, range(0, seqlen(512)*headdim(64))
    xoffset0 = pid0*XBLOCK0                     # XBLOCK0 is the "height" of block (see beginning), `BLOCK_M` in flex
    x0 = xoffset0 + tl.arange(0, XBLOCK0)       # block idx for rows (along seqlen dim of Q), `offs_m` in flex
    x6 = tl.arange(0, 64)                       # block idx for headdim(64) of V, flex: 26, V_HEAD_DIM=64
    xmask = tl.full([XBLOCK0], True, tl.int1)   # unused, original TorchInductor generated legacy
    # x4 = (xindex // 512) % 32                 # legacy code (xindex generated by original TorchInductor)
    tmp22 = tl.full([XBLOCK0, 1], float("-inf"), tl.float32)    # max loop-carried accumulator, flex: 110, m_i
    # x5 = (xindex // 16384)                    # legacy code
    tmp40 = tl.full([XBLOCK0, 64], 0, tl.float32)               # final output buffer, flex: 112, acc
    _tmp48 = tl.full([XBLOCK0, RBLOCK], 0, tl.float32)          # sum loop-carried accumulator, flex: 111, l_i; NOTE shape could have been [XBLOCK0, 1], using RBLOCK for multi-lane accumulation
    rbase = tl.arange(0, RBLOCK)                # block idx for columns, RBLOCK is `BLOCK_N` in flex
    for roffset in range(0, 512, RBLOCK):       # loop over seqlen(512) of K/V, flex: 280
        rindex = roffset + rbase                # block idx for columns in this iteration
        rmask = rindex < 512                    # mask to make sure column does not exceed seqlen(512)
        r7 = rindex                             # block idx for columns in this iteration, `offs_n` in flex
        # x0 = xindex % 512                     # legacy
        # x1 = (xindex // 512)                  # legacy
        tmp5 = tl.full([XBLOCK0, RBLOCK], 0, tl.float32)    # Q@K block buffer (can be eliminated)
        rbase1 = tl.arange(0, RBLOCK1)          # Q@K blocked mm inner idx
        # for roffset1 in range(0, 64, RBLOCK1):    # Q@K could have multiple blocks along inner dim, but headdim=64 is small and one block is enough, so eliminate this nested loop
        #     rindex1 = roffset1 + rbase1           # eliminated with the loop
        rindex1 = rbase1                        # Q@K blocked mm inner idx
        rmask1 = rindex1 < 64                   # unused mask
        r8 = rindex1                            # Q@K blocked mm inner idx
        tmp0 = tl.load(in_ptr0 + ((r8)[None, :] + (64*x0)[:, None] + 32768*x1), (r8 < 64)[None, :] & (x0 < 512)[:, None] & (x1 < 1024), eviction_policy='evict_last', other=0.0)    # indexing Q with (x1, x0, r8)
        tmp1 = tl.load(in_ptr1 + ((r8)[:, None] + (64*r7)[None, :] + 32768*x1), (r8 < 64)[:, None] & (r7 < 512)[None, :] & (x1 < 1024), eviction_policy='evict_last', other=0.0)    # indexing K with (x1, r8, r7)
        tmp2 = tmp0.to(tl.bfloat16)     # bf16 to use tensor core
        tmp3 = tmp1.to(tl.bfloat16)     # bf16 to use tensor core
        tmp4 = tl.dot(tmp2, tmp3)       # Q@K, flex: 367, qk
        tmp6 = tmp5 + tmp4            # Q@K accumulation (there used to be a loop, see line 62); tmp5 is 0 and can be eliminated
        tmp5 = tmp6                   # update the buffer, now tmp5 is Q@K
        tmp35 = tl.load(in_ptr2 + ((x6)[None, :] + (64*r7)[:, None] + 32768*x1), (x6 < 64)[None, :] & (r7 < 512)[:, None] & (x1 < 1024), eviction_policy='evict_last', other=0.0)   # indexing V with (x1, r7, r6)
        tmp7 = 0.125                            # 1/sqrt(headdim=64)
        tmp8 = tmp5 * tmp7                      # scale Q@K -> Q@K/sqrt(d)
        tmp9 = tmp8.to(tl.float32)              # alibi bias starts, flex: 381, tmp0
        tmp10 = 1 + x4                          # flex: 384-385, tmp4
        tmp11 = tmp10.to(tl.float32)            # flex: 386, tmp5
        tmp12 = 8.0                             # flex: 387, tmp6
        tmp13 = tmp11 * tmp12                   # flex: 388, tmp7
        tmp14 = 0.03125                         # flex: 389, tmp8
        tmp15 = tmp13 * tmp14                   # flex: 390, tmp9
        tmp16 = -tmp15                          # flex: 391, tmp10
        tmp17 = libdevice.exp2(tmp16)           # flex: 392, tmp11
        tmp18 = (r7)[None, :] + (-x0)[:, None]  # flex: 382, tmp1
        tmp19 = tmp18.to(tl.float32)            # flex: 383, tmp2
        tmp20 = tmp17 * tmp19                   # flex: 393, tmp12
        tmp21 = tmp9 + tmp20                    # alibi bias ends, flex: 394-395, tmp13/post_mod_scores
        tmp23 = triton_helpers.max2(tmp21, 1)                   # now tmp21 is the biased QK block, do max over columns, `1` means `dim=1`, flex: 418, m_ij
        tmp24 = triton_helpers.maximum(tmp22, tmp23[:, None])   # update loop-carried max accumulator, flex: 418
        tmp25 = 1 + (x1 % 32)           # (known issue) this repeats line 78 , to be eliminated
        tmp26 = tmp25.to(tl.float32)    # (known issue) this repeats line 79, to be eliminated
        tmp27 = tmp26 * tmp12           # (known issue) this repeats line 81, to be eliminated
        tmp28 = tmp27 * tmp14           # (known issue) this repeats line 83, to be eliminated
        tmp29 = -tmp28                  # (known issue) this repeats line 84, to be eliminated
        tmp30 = libdevice.exp2(tmp29)   # (known issue) this repeats line 85, to be eliminated
        tmp31 = tmp30 * tmp19           # (known issue) this repeats line 88, to be eliminated
        tmp32 = tmp9 + tmp31            # (known issue) this repeats line 89, to be eliminated
        tmp33 = tmp32 - tmp24           # biased_qk - m, flex: 426, post_mod_scores - m_ij_masked[:, None]
        tmp34 = tl_math.exp(tmp33)      # e^(qk-m), flex: 426, tl.math.exp2(post_mod_scores - m_ij_masked[:, None])
        tmp36 = tmp35.to(tl.float32)    # V block
        tmp37 = tmp34.to(tl.float32)    # e^(qk-m)
        tmp38 = tmp36.to(tl.float32)    # V block
        tmp39 = tl.dot(tmp37, tmp38)    # (e^(qk-m))@V, flex: 439, acc
        tmp41 = tmp22 - tmp24           # old max - new max, flex: 425, m_i - m_ij_masked
        tmp42 = tl_math.exp(tmp41)      # renew factor, flex: 425, alpha
        tmp43 = tmp40 * tmp42           # renew output acc, flex: 433, acc * alpha[:, None]
        tmp40 = tmp43                   # (redundant) write back to output acc
        tmp44 = tmp40 + tmp39           # accumulate (e^(qk-m))@V, flex: 439, acc passed as accumulator argument
        tmp40 = tmp44                   # write back to output acc, flex: 439
        tmp45 = tmp21 - tmp24           # (known issue) this repeats line 100, to be eliminated
        tmp46 = tl_math.exp(tmp45)      # (known issue) this repeats line 101, to be eliminated
        tmp47 = tl.broadcast_to(tmp46, [XBLOCK0, RBLOCK])   # this block's e^(qk-m), the shape is already [XBLOCK0, RBLOCK]
        tmp49 = _tmp48 * tmp42          # renew old sum, flex: 431, l_i * alpha
        _tmp48 = tmp49                  # (redundant) write back to sum acc
        tmp50 = _tmp48 + tmp47          # add sum of this block, flex: 431, l_i = l_i * alpha + tl.sum(p, 1)
        _tmp48 = tl.where(rmask[None, :], tmp50, _tmp48)    # write back to sum acc, flex: 431
        tmp22 = tmp24   # write back to max acc, flex: 442, m_i = m_ij
    tmp48 = tl.sum(_tmp48, 1)[:, None]  # we are doing multi-lane sum accumulation; sum the lanes
    tmp51 = tmp40 / tmp48   # apply the denominator, flex: 219, acc / l_i
    tl.store(in_out_ptr0 + ((x6)[None, :] + (64*x0)[:, None] + 32768*x1), tmp51, (x6 < 64)[None, :] & (x0 < 512)[:, None] & (x1 < 1024))    # indexing output with (x1, x0, r6)
